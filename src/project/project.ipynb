{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "430dd179-36b3-4338-a11b-90f080a3046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, col, when, sum\n",
    "\n",
    "import pyodbc\n",
    "\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee071b0-eeda-4673-9b93-bf0c2cbe05f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('172.30.1.44', 'APT_Project', 'APT_Project', 'APT_Project')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getenv('host_ip'),os.getenv('DATABASE'),os.getenv('user_id'),os.getenv('user_password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa8173be-8ccc-4906-8fa0-d931a50ef95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SQL Server', 'ODBC Driver 17 for SQL Server', 'SQL Server Native Client RDA 11.0']\n"
     ]
    }
   ],
   "source": [
    "# 설치된 드라이버 확인\n",
    "print(pyodbc.drivers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "193bb7a9-0c56-40c3-8eee-28283ab16600",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "config = {\n",
    "    'server' : os.getenv('host_ip'),\n",
    "    'database' : os.getenv('DATABASE'),\n",
    "    'username' : os.getenv('user_id'),\n",
    "    'password' : os.getenv('user_password'),\n",
    "    'driver' : 'ODBC Driver 17 for SQL Server',\n",
    "}\n",
    "\n",
    "conn_str = f\"DRIVER={config['driver']};SERVER={config['server']};DATABASE={config['database']};UID={config['username']};PWD={config['password']}\"\n",
    "conn = pyodbc.connect(conn_str)\n",
    "\n",
    "conn\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "748f7b0f-fd13-42d7-8674-4ed0667b6b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2017', '11680', '강남구', '11000', '압구정동', '1', '대지', '369', '1', '현대1차(12,13,21,22,31,32,33동)', '209000', '131.48', 0.0, '12', None, None, '1976', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11320', '도봉구', '10700', '창동', '1', '대지', '6', '1', '\\t(6-1)\\t', '9900', '37.28', 48.11, '7', None, None, '1997', '오피스텔', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11440', '마포구', '10400', '도화동', '1', '대지', '555', '0', '한화오벨리스크', '34500', '38.08', 0.0, '14', None, None, '2004', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11545', '금천구', '10200', '독산동', '1', '대지', '711', '2', '금천현대', '30000', '59.82', 0.0, '14', None, None, '2002', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11380', '은평구', '11400', '진관동', '1', '대지', '140', '0', '폭포동4단지힐스테이트(408~422동)BL3-9', '76767', '166.96', 0.0, '5', '분양권', None, '0   ', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11650', '서초구', '10100', '방배동', '1', '대지', '769', '7', '(769-7)', '50000', '56.68', 33.32, '2', None, None, '2016', '연립다세대', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11320', '도봉구', '10600', '방학동', '1', '대지', '731', '0', '한화성원아파트', '29000', '84.98', 0.0, '15', None, None, '1996', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11380', '은평구', '10200', '녹번동', '1', '대지', '280', '1', '녹번JR아파트', '32500', '59.87', 0.0, '4', None, None, '2002', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11350', '노원구', '10400', '하계동', '1', '대지', '274', '0', '청솔(시영7)', '25400', '49.5', 0.0, '4', None, None, '1989', '아파트', None, None, datetime.date(2017, 5, 1))\n",
      "('2017', '11200', '성동구', '10800', '응봉동', '1', '대지', '100', '0', '대림(1차)', '51800', '75.51', 0.0, '4', None, None, '1986', '아파트', None, None, datetime.date(2017, 5, 1))\n"
     ]
    }
   ],
   "source": [
    "for i in cursor.execute('select * from tbAPTPrice').fetchmany(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f20df3b9-01a6-4248-b057-53b36767ba62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDBC URL: jdbc:sqlserver://172.30.1.44:1433;databaseName=APT_Project;\n",
      "JDBC Properties: {'user': 'APT_Project', 'password': 'APT_Project', 'driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver'}\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'server' : os.getenv('host_ip'),\n",
    "    'database' : os.getenv('DATABASE'),\n",
    "    'username' : os.getenv('user_id'),\n",
    "    'password' : os.getenv('user_password'),\n",
    "    'driver' : 'ODBC Driver 17 for SQL Server',\n",
    "}\n",
    "# JDBC URL 문자열 생성\n",
    "jdbc_url = f\"jdbc:sqlserver://{config['server']}:1433;databaseName={config['database']};\"\n",
    "\n",
    "# Spark JDBC 연결 설정\n",
    "properties = {\n",
    "    \"user\": config['username'],\n",
    "    \"password\": config['password'],\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "print(\"JDBC URL:\", jdbc_url)\n",
    "print(\"JDBC Properties:\", properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acc7ff98-b70f-4afe-ab1f-3dc9401b0991",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1120)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1106)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:563)\r\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$13(Executor.scala:953)\r\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$13$adapted(Executor.scala:945)\r\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\r\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\r\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\r\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\r\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\r\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:945)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:247)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:579)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# SparkSession 생성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m ss \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPySpark MSSQL Example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile:/C:/Users/YJ/Desktop/AllProjects/sqljdbc_12.8/kor/jars/mssql-jdbc-12.8.1.jre8.jar\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark Session이 성공적으로 생성되었습니다!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\sql\\session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    226\u001b[0m         sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m     sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    231\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\context.py:384\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\context.py:146\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    144\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparkHome\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpyFiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menvironment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjsc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\context.py:209\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_context\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\context.py:321\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_initialize_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, jconf):\n\u001b[0;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    Initialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mJavaSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\py4j\\java_gateway.py:1568\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1562\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1564\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1565\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1567\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1568\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1572\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1120)\r\n\tat org.apache.hadoop.fs.FileUtil.chmod(FileUtil.java:1106)\r\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:563)\r\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$13(Executor.scala:953)\r\n\tat org.apache.spark.executor.Executor.$anonfun$updateDependencies$13$adapted(Executor.scala:945)\r\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\r\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\r\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\r\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\r\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\r\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\r\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\r\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:945)\r\n\tat org.apache.spark.executor.Executor.<init>(Executor.scala:247)\r\n\tat org.apache.spark.scheduler.local.LocalEndpoint.<init>(LocalSchedulerBackend.scala:64)\r\n\tat org.apache.spark.scheduler.local.LocalSchedulerBackend.start(LocalSchedulerBackend.scala:132)\r\n\tat org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:220)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:579)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SparkSession 생성\n",
    "ss = SparkSession.builder \\\n",
    "    .appName(\"PySpark MSSQL Example\") \\\n",
    "    .config(\"spark.jars\", \"file:/C:/Users/YJ/Desktop/AllProjects/sqljdbc_12.8/kor/jars/mssql-jdbc-12.8.1.jre8.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session이 성공적으로 생성되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8cb2476-7487-4e36-a80a-dcd6817a8324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JDBC URL: jdbc:sqlserver://172.30.1.44:1433;databaseName=APT_Project;\n",
      "JDBC Properties: {'user': 'APT_Project', 'password': 'APT_Project', 'driver': 'com.microsoft.sqlserver.jdbc.SQLServerDriver'}\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o152.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:340)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 31\u001b[0m\n\u001b[0;32m     23\u001b[0m ss \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPySpark MSSQL Example\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/YJ/Desktop/AllProjects/sqljdbc_12.8/kor/jars\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     28\u001b[0m ss\n\u001b[1;32m---> 31\u001b[0m \u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mjdbc_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdbo.tbAPTPrice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m    \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m)\u001b[49m\\\n\u001b[0;32m     36\u001b[0m \u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:871\u001b[0m, in \u001b[0;36mDataFrameReader.jdbc\u001b[1;34m(self, url, table, column, lowerBound, upperBound, numPartitions, predicates, properties)\u001b[0m\n\u001b[0;32m    869\u001b[0m     jpredicates \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mtoJArray(gateway, gateway\u001b[38;5;241m.\u001b[39mjvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mString, predicates)\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mjdbc(url, table, jpredicates, jprop))\n\u001b[1;32m--> 871\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\.conda\\envs\\django\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o152.jdbc.\n: java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver\r\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:593)\r\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\r\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\r\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\r\n\tat org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:340)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'server' : os.getenv('host_ip'),\n",
    "    'database' : os.getenv('DATABASE'),\n",
    "    'username' : os.getenv('user_id'),\n",
    "    'password' : os.getenv('user_password'),\n",
    "    'driver' : 'ODBC Driver 17 for SQL Server',\n",
    "}\n",
    "# JDBC URL 문자열 생성\n",
    "jdbc_url = f\"jdbc:sqlserver://{config['server']}:1433;databaseName={config['database']};\"\n",
    "\n",
    "# Spark JDBC 연결 설정\n",
    "properties = {\n",
    "    \"user\": config['username'],\n",
    "    \"password\": config['password'],\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "print(\"JDBC URL:\", jdbc_url)\n",
    "print(\"JDBC Properties:\", properties)\n",
    "ss\n",
    "\n",
    "\n",
    "ss.read.jdbc(\n",
    "    url = jdbc_url,\n",
    "    table = 'dbo.tbAPTPrice',\n",
    "    properties = properties    \n",
    ")\\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6602b-09d5-43ac-a4ef-08030e70e7b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613f3dc8-ea21-42e3-8882-cccd2983947d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0c7df5-a6d0-4e4d-9b04-c4effc6f1574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29f35a-4fb9-4983-a8db-fa0cbccb2048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47034b30-0fb3-4fee-ac4a-9d368bc11354",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1a7e9e-80ca-442f-88fb-fc41489eeb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595ed87-dc8b-4675-ba85-d76ee2ba3811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f197447-9b14-428d-a264-d1e88487804a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf8a73-fd7a-4183-b628-e658ffcded56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8637ed95-8665-45aa-b45e-42a51ac99acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-H62JI1T:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ca9016f310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba375f7-c685-4997-9e98-bca9249117ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(os.getcwd(), 'data', 'house_project_data/*.csv')\n",
    "file_path2 = os.path.join(os.getcwd(), 'data', 'house_project_data/code/*.csv')\n",
    "\n",
    "df = ss.read.csv(f'file:///{file_path}', inferSchema = True, header = True, encoding = 'cp949')\n",
    "df\n",
    "\n",
    "df_rate = ss.read.csv(f'file:///{file_path2}', inferSchema = True, header = True, encoding = 'cp949')\n",
    "df_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f12a7ee3-f931-472c-a11f-58eec309cbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 로컬용\n",
    "tmp_df1 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2017.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df2 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2018.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df3 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2019.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df4 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2020.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df5 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2021.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df6 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2022.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df7 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2023.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "tmp_df8 = ss.read.csv('./data/house_project_data/서울시 부동산 실거래가 정보_2024.csv', inferSchema = True, header = True, encoding = 'cp949')\n",
    "\n",
    "df = tmp_df1\\\n",
    "    .union(tmp_df2)\\\n",
    "    .union(tmp_df3)\\\n",
    "    .union(tmp_df4)\\\n",
    "    .union(tmp_df5)\\\n",
    "    .union(tmp_df6)\\\n",
    "    .union(tmp_df7)\\\n",
    "    .union(tmp_df8)\n",
    "\n",
    "df_rate = ss.read.csv('./data/house_project_data/code/korea_base_interest_rate.csv', inferSchema = True, header = True, encoding = 'cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "480ec27e-525a-4a25-ba02-c575860aefc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Row(접수연도=2017, 자치구코드=11740, 자치구명='강동구', 법정동코드=10900, 법정동명='천호동', 지번구분=1, 지번구분명='대지', 본번='0449', 부번='49', 건물명='힐탑프라자', 계약일=20180310, 물건금액(만원)=8900, 건물면적(㎡)=20.48, 토지면적(㎡)=24.26, 층=16, 권리구분=None, 취소일=None, 건축년도='1994', 건물용도='오피스텔', 신고구분=None, 신고한 개업공인중개사 시군구명=None),\n",
       "  Row(접수연도=2017, 자치구코드=11260, 자치구명='중랑구', 법정동코드=10400, 법정동명='묵동', 지번구분=1, 지번구분명='대지', 본번='0243', 부번='110', 건물명='애플하우스', 계약일=20180307, 물건금액(만원)=16500, 건물면적(㎡)=29.7, 토지면적(㎡)=16.84, 층=4, 권리구분=None, 취소일=None, 건축년도='2012', 건물용도='연립다세대', 신고구분=None, 신고한 개업공인중개사 시군구명=None),\n",
       "  Row(접수연도=2017, 자치구코드=11680, 자치구명='강남구', 법정동코드=11400, 법정동명='일원동', 지번구분=1, 지번구분명='대지', 본번='0731', 부번='0', 건물명='한솔마을', 계약일=20180206, 물건금액(만원)=135000, 건물면적(㎡)=84.73, 토지면적(㎡)=0.0, 층=5, 권리구분=None, 취소일=None, 건축년도='1994', 건물용도='아파트', 신고구분=None, 신고한 개업공인중개사 시군구명=None)],\n",
       " [Row(접수연도=2024, 자치구코드=11530, 자치구명='구로구', 법정동코드=10700, 법정동명='개봉동', 지번구분=1, 지번구분명='대지', 본번='0170', 부번='0033', 건물명='금석연립(170-33)', 계약일=20220708, 물건금액(만원)=9740, 건물면적(㎡)=53.61, 토지면적(㎡)=124.0, 층=2, 권리구분=None, 취소일='20240207', 건축년도='1981', 건물용도='연립다세대', 신고구분='중개거래', 신고한 개업공인중개사 시군구명='서울 강남구'),\n",
       "  Row(접수연도=2024, 자치구코드=11440, 자치구명='마포구', 법정동코드=11000, 법정동명='노고산동', 지번구분=None, 지번구분명=None, 본번=None, 부번=None, 건물명=None, 계약일=20210719, 물건금액(만원)=195000, 건물면적(㎡)=157.45, 토지면적(㎡)=116.7, 층=None, 권리구분=None, 취소일='20240417', 건축년도='1988', 건물용도='단독다가구', 신고구분=None, 신고한 개업공인중개사 시군구명=None),\n",
       "  Row(접수연도=2024, 자치구코드=11305, 자치구명='강북구', 법정동코드=10100, 법정동명='미아동', 지번구분=None, 지번구분명=None, 본번=None, 부번=None, 건물명=None, 계약일=20210208, 물건금액(만원)=164000, 건물면적(㎡)=293.08, 토지면적(㎡)=136.0, 층=None, 권리구분=None, 취소일='20240905', 건축년도='1994', 건물용도='단독다가구', 신고구분=None, 신고한 개업공인중개사 시군구명=None)],\n",
       " [Row(date='2024-11-28', base_interest_rate=3.0),\n",
       "  Row(date='2024-10-11', base_interest_rate=3.25),\n",
       "  Row(date='2023-01-13', base_interest_rate=3.5)],\n",
       " [Row(date='2019-07-18', base_interest_rate=1.5),\n",
       "  Row(date='2018-11-30', base_interest_rate=1.75),\n",
       "  Row(date='2017-11-30', base_interest_rate=1.5)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3), df.tail(3), df_rate.head(3), df_rate.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a9c0c1-7bd3-45ca-8f7d-f05b1918e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------+----------+--------+--------+----------+----+----+----------+--------+--------------+------------+------------+---+--------+------+--------+----------+--------+------------------------------+\n",
      "|접수연도|자치구코드|자치구명|법정동코드|법정동명|지번구분|지번구분명|본번|부번|건물명    |계약일  |물건금액(만원)|건물면적(㎡)|토지면적(㎡)|층 |권리구분|취소일|건축년도|건물용도  |신고구분|신고한 개업공인중개사 시군구명|\n",
      "+--------+----------+--------+----------+--------+--------+----------+----+----+----------+--------+--------------+------------+------------+---+--------+------+--------+----------+--------+------------------------------+\n",
      "|2017    |11740     |강동구  |10900     |천호동  |1       |대지      |0449|49  |힐탑프라자|20180310|8900          |20.48       |24.26       |16 |null    |null  |1994    |오피스텔  |null    |null                          |\n",
      "|2017    |11260     |중랑구  |10400     |묵동    |1       |대지      |0243|110 |애플하우스|20180307|16500         |29.7        |16.84       |4  |null    |null  |2012    |연립다세대|null    |null                          |\n",
      "|2017    |11680     |강남구  |11400     |일원동  |1       |대지      |0731|0   |한솔마을  |20180206|135000        |84.73       |0.0         |5  |null    |null  |1994    |아파트    |null    |null                          |\n",
      "+--------+----------+--------+----------+--------+--------+----------+----+----+----------+--------+--------------+------------+------------+---+--------+------+--------+----------+--------+------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d595ba5-a533-4cf7-a979-92c2c0ae1fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 접수연도: integer (nullable = true)\n",
      " |-- 자치구코드: integer (nullable = true)\n",
      " |-- 자치구명: string (nullable = true)\n",
      " |-- 법정동코드: integer (nullable = true)\n",
      " |-- 법정동명: string (nullable = true)\n",
      " |-- 지번구분: integer (nullable = true)\n",
      " |-- 지번구분명: string (nullable = true)\n",
      " |-- 본번: string (nullable = true)\n",
      " |-- 부번: string (nullable = true)\n",
      " |-- 건물명: string (nullable = true)\n",
      " |-- 계약일: integer (nullable = true)\n",
      " |-- 물건금액(만원): integer (nullable = true)\n",
      " |-- 건물면적(㎡): double (nullable = true)\n",
      " |-- 토지면적(㎡): double (nullable = true)\n",
      " |-- 층: integer (nullable = true)\n",
      " |-- 권리구분: string (nullable = true)\n",
      " |-- 취소일: string (nullable = true)\n",
      " |-- 건축년도: string (nullable = true)\n",
      " |-- 건물용도: string (nullable = true)\n",
      " |-- 신고구분: string (nullable = true)\n",
      " |-- 신고한 개업공인중개사 시군구명: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0364dec1-3c33-4983-a962-681e33bbce32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1035346"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "519f6fb6-c4bb-4868-923a-26c96c35ff9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sumbit_year',\n",
       " 'jachigu_code',\n",
       " 'jachigu_name',\n",
       " 'law_code',\n",
       " 'law_name',\n",
       " 'jibun_gubun',\n",
       " 'jibun_name',\n",
       " 'real_num',\n",
       " 'extra_num',\n",
       " 'building_name',\n",
       " 'contract_date',\n",
       " 'building_price',\n",
       " 'building_size',\n",
       " 'land_size',\n",
       " 'floor',\n",
       " 'right_gubun',\n",
       " 'cancel_date',\n",
       " 'building_year',\n",
       " 'building_usage',\n",
       " 'singo_gubun',\n",
       " 'agent_area']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_columns = ['sumbit_year', 'jachigu_code', 'jachigu_name', 'law_code', 'law_name', 'jibun_gubun', 'jibun_name', 'real_num', 'extra_num', 'building_name', 'contract_date', 'building_price', 'building_size', 'land_size', 'floor', 'right_gubun', 'cancel_date', 'building_year', 'building_usage', 'singo_gubun', 'agent_area']\n",
    "\n",
    "if len(df.columns) == len(renamed_columns):\n",
    "    renamed_columns_dict = {i : j for i,j in zip(df.columns, renamed_columns)}\n",
    "\n",
    "for num, i in enumerate(df.columns):\n",
    "    if i == list(renamed_columns_dict.keys())[num]:\n",
    "        df = df.withColumnRenamed(i, list(renamed_columns_dict.values())[num])\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0987f-7013-4dcf-8347-2df8eabda50e",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59806dd-479f-475f-aeb1-2c499bda58d9",
   "metadata": {},
   "source": [
    "### 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "eca90c9b-a055-4594-b2ed-d4140771380c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sumbit_year_nulls': 0,\n",
       " 'jachigu_code_nulls': 0,\n",
       " 'jachigu_name_nulls': 1,\n",
       " 'law_code_nulls': 0,\n",
       " 'law_name_nulls': 0,\n",
       " 'jibun_gubun_nulls': 74005,\n",
       " 'jibun_name_nulls': 74005,\n",
       " 'real_num_nulls': 73993,\n",
       " 'extra_num_nulls': 73993,\n",
       " 'building_name_nulls': 73962,\n",
       " 'contract_date_nulls': 0,\n",
       " 'building_price_nulls': 0,\n",
       " 'building_size_nulls': 0,\n",
       " 'land_size_nulls': 179035,\n",
       " 'floor_nulls': 73911,\n",
       " 'right_gubun_nulls': 1018839,\n",
       " 'cancel_date_nulls': 1008908,\n",
       " 'building_year_nulls': 0,\n",
       " 'building_usage_nulls': 0,\n",
       " 'singo_gubun_nulls': 795534,\n",
       " 'agent_area_nulls': 838902}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = df.select(\n",
    "        [sum(\n",
    "            when(\n",
    "                col(i).isNull() | isnan(i),1)\\\n",
    "            .otherwise(0)\n",
    "            )\\\n",
    "            .alias(i + '_nulls') for i in df.columns\n",
    "        ]\n",
    "    ).collect()\n",
    "    \n",
    "tmp[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e966b8c8-a999-41aa-9179-575960afa45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04817179-905f-45e4-8491-014fc2a17b9c",
   "metadata": {},
   "source": [
    "### 컬럼별 결측치 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4c1ce419-3eb1-4765-a333-6a8de0400886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|jachigu_name|\n",
      "+------------+\n",
      "|        null|\n",
      "+------------+\n",
      "\n",
      "+------------+\n",
      "|jachigu_name|\n",
      "+------------+\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('jachigu_name')\\\n",
    "    .where('jachigu_name IS NULL')\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "# 확인 결과 자치구명 NULL의 1건은 미아동이며, 2020년 2차 행복주택 입주자 모집에 따르면 미아동이 맞음을 확인할 수 있음\n",
    "# 이에 다른 미아동처럼 강북구 코드인 11305으로 업데이트\n",
    "# 출처 : https://image.ebunyang.co.kr/files/bunyang/pdf/uploadfile_202012283766287.pdf\n",
    "# 출처 : https://www.code.go.kr/stdcode/regCodeL.do\n",
    "df = df.select('jachigu_name')\\\n",
    "        .where('jachigu_name IS NULL')\\\n",
    "        .fillna('11305')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ab715-96e5-4833-a4f1-32c1d38be4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('tmp')\n",
    "\n",
    "ss.sql('''\n",
    "    SELECT\n",
    "        *\n",
    "    \n",
    "    FROM tmp\n",
    "    WHERE jachigu_name is NULL\n",
    "\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5370f6e9-0d78-4a3d-aa10-b38f70a4cf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b7bde-6eab-487c-8a77-29a445d71e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d6e871-b436-430c-9065-e38b037c679c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d777ff-80dd-4556-a64f-1efff9c5e2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818b832-f2a4-4a2e-979b-5bde542669cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b355696f-9b73-46e8-882f-a0a979568bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'my_jupyter'",
   "language": "python",
   "name": "ipykernel1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
