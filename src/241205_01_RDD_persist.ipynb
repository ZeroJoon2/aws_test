{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b3de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 10:24:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-3-9.ap-northeast-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_app</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=spark_app>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import os\n",
    "conf = SparkConf().setMaster('local').setAppName('spark_app')\n",
    "spark = SparkContext(conf = conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e09d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory = ''\n",
    "directory = os.path.join(os.getcwd(), 'data')\n",
    "file_name = 'restaurant_reviews.csv'\n",
    "file_path = os.path.join(directory, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc7545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['id,item,cateogry,reviews,', '0,짜장면,중식,125,', '1,짬뽕,중식,235,']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.textFile('file:///' + file_path.replace('\\\\','/'))\n",
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1269e5-f47d-44f0-a8cb-cfda0eed84ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'id,item,cateogry,reviews,'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = lines.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51cce8a-71e1-453d-95dd-ce16d07cbb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,짜장면,중식,125,', '1,짬뽕,중식,235,', '2,김밥,분식,32,']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lines = lines.filter(lambda row : row != header)\n",
    "filtered_lines.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "467d3f3d-7a66-47ae-ac06-0a3b5ca53821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(row):\n",
    "    field = row.split(',')\n",
    "    category = field[2]\n",
    "    reviews = int(field[3])\n",
    "\n",
    "    return category, reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21584f47-6b32-44fd-bc80-42a3402538be",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = filtered_lines.map(parse).collect()\n",
    "res2 = filtered_lines.map(parse).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f31a07b4-11c0-47d1-a947-f3e5e95ac5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('중식', 125),\n",
       "  ('중식', 235),\n",
       "  ('분식', 32),\n",
       "  ('분식', 534),\n",
       "  ('일식', 223),\n",
       "  ('일식', 52),\n",
       "  ('일식', 12),\n",
       "  ('아시안', 312),\n",
       "  ('패스트푸드', 12),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23)],\n",
       " [('중식', 125), ('중식', 235), ('분식', 32), ('분식', 534), ('일식', 223)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db2b6735-5d27-45e2-9aca-63baa52bd1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[19] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_reviews = filtered_lines.map(parse).persist() #persist를 통해 매번 캐시에 올리지 않음\n",
    "cat_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2987d33-cfc5-49e7-9545-7120f4b12a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = cat_reviews.collect()\n",
    "res2 = cat_reviews.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0c36bd4-fa06-4210-ae50-5039f9e116a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('중식', 125),\n",
       "  ('중식', 235),\n",
       "  ('분식', 32),\n",
       "  ('분식', 534),\n",
       "  ('일식', 223),\n",
       "  ('일식', 52),\n",
       "  ('일식', 12),\n",
       "  ('아시안', 312),\n",
       "  ('패스트푸드', 12),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23),\n",
       "  ('패스트푸드', 23)],\n",
       " [('중식', 125),\n",
       "  ('중식', 235),\n",
       "  ('분식', 32),\n",
       "  ('분식', 534),\n",
       "  ('일식', 223),\n",
       "  ('일식', 52),\n",
       "  ('일식', 12),\n",
       "  ('아시안', 312),\n",
       "  ('패스트푸드', 12),\n",
       "  ('패스트푸드', 23)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1, res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a02bab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0e7c2d-530e-494c-9ee9-9b5e44b99c26",
   "metadata": {},
   "source": [
    "A와 B의 차이가 뭐니?\n",
    "\n",
    "[A]\n",
    "res1 = filtered_lines.map(parse)\n",
    "res1.take(3)\n",
    "\n",
    "[B]\n",
    "res1 = filtered_lines.map(parse).persist()\n",
    "res1.take(3)\n",
    "ChatGPT의 말:\n",
    "ChatGPT\n",
    "Spark에서 [A]와 [B]의 차이는 **RDD의 캐싱(메모리에 저장)**과 관련이 있습니다. 이를 상세히 설명하겠습니다.\n",
    "\n",
    "[A] res1 = filtered_lines.map(parse); res1.take(3)\n",
    "이 코드는 RDD 연산을 실행할 때마다 계산을 반복합니다.\n",
    "res1은 map(parse) 트랜스포메이션에 의해 생성된 PipelinedRDD입니다.\n",
    "res1.take(3)를 호출하면:\n",
    "filtered_lines의 모든 데이터를 대상으로 map(parse) 연산이 수행됩니다.\n",
    "연산이 완료되면 결과 중 상위 3개의 값을 반환합니다.\n",
    "트랜스포메이션 재실행:\n",
    "res1.take(3)를 호출할 때마다 map(parse) 연산이 다시 실행됩니다. 즉, 동일한 데이터를 반복적으로 계산합니다.\n",
    "성능 저하가 발생할 수 있습니다.\n",
    "[B] res1 = filtered_lines.map(parse).persist(); res1.take(3)\n",
    "이 코드는 persist() 메서드를 사용해 RDD를 메모리 또는 디스크에 저장합니다.\n",
    "persist()를 호출하면:\n",
    "res1의 첫 번째 액션(take, collect, count 등)이 실행될 때, Spark는 데이터를 계산하고 RDD의 중간 결과를 캐싱합니다.\n",
    "이후 동일한 RDD(res1)에 대해 액션을 호출하면 캐싱된 데이터를 재사용합니다.\n",
    "트랜스포메이션 실행 횟수 감소:\n",
    "한 번 계산된 데이터를 재사용하므로, 동일한 데이터에 대한 반복적인 트랜스포메이션 수행이 방지됩니다.\n",
    "성능 향상을 제공합니다.\n",
    "주요 차이점 요약\n",
    "특징\t[A] 기본 연산\t[B] persist() 사용\n",
    "RDD 재계산\t매번 트랜스포메이션이 재계산됩니다.\t첫 번째 액션 이후, 캐싱된 데이터를 재사용합니다.\n",
    "성능\t반복적으로 데이터를 읽고 처리하므로 느릴 수 있습니다.\t캐싱된 데이터를 사용하므로 반복 작업이 빠릅니다.\n",
    "메모리 사용\t추가적인 메모리나 디스크 자원이 사용되지 않습니다.\t메모리나 디스크에 데이터가 저장됩니다.\n",
    "데이터 재사용\t동일한 데이터를 반복적으로 계산합니다.\t동일한 데이터를 효율적으로 재사용합니다.\n",
    "언제 [A]와 [B]를 사용할까?\n",
    "[A] 사용:\n",
    "RDD가 단일 연산으로 한 번만 사용되고, 결과를 재사용할 필요가 없을 때.\n",
    "[B] 사용:\n",
    "동일한 RDD에 대해 여러 번 액션을 호출하거나 결과를 반복적으로 사용할 때.\n",
    "대량 데이터 처리에서 성능 최적화가 필요할 때.\n",
    "persist()와 cache()의 관계\n",
    "persist()는 데이터 저장 방식을 명시적으로 지정할 수 있습니다(예: 메모리, 디스크, 또는 혼합).\n",
    "cache()는 persist(StorageLevel.MEMORY_AND_DISK)의 별칭으로, 데이터를 메모리에 저장하며, 메모리가 부족하면 디스크에 저장합니다.\n",
    "예제 비교\n",
    "[A]:\n",
    "python\n",
    "코드 복사\n",
    "res1 = filtered_lines.map(parse)\n",
    "print(res1.take(3))  # 첫 번째 트랜스포메이션 실행\n",
    "print(res1.take(3))  # 다시 트랜스포메이션 실행 (중복 계산 발생)\n",
    "[B]:\n",
    "python\n",
    "코드 복사\n",
    "res1 = filtered_lines.map(parse).persist()\n",
    "print(res1.take(3))  # 첫 번째 트랜스포메이션 실행 및 데이터 캐싱\n",
    "print(res1.take(3))  # 캐싱된 데이터 사용 (중복 계산 없음)\n",
    "성능 차이:\n",
    "[A]는 두 번의 take 호출로 동일한 데이터를 두 번 계산.\n",
    "[B]는 첫 번째 호출에서 데이터를 캐싱하고 두 번째 호출은 캐싱된 데이터를 사용.\n",
    "결론\n",
    "[A]는 메모리를 절약하지만, 반복적인 계산이 필요한 경우 성능이 저하됩니다.\n",
    "[B]는 메모리를 더 사용하지만, 반복 작업에서 성능이 향상됩니다.\n",
    "대량 데이터 처리와 반복 작업이 필요한 상황에서는 [B]를 추천합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05498955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3080d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af898725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spark_start)",
   "language": "python",
   "name": "spark_start"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
