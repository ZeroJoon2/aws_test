{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a9ccfb9a-a64a-493a-93e9-8ef64685c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-3-9.ap-northeast-3.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>reduce</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=reduce>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster('local').setAppName('reduce')\n",
    "spark = SparkContext(conf = conf).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f7213-697c-46c5-929c-088b237e66d6",
   "metadata": {},
   "source": [
    "### Reduce : 여러개의 값을 하나로 축약\n",
    "RDD.reduce([<함수>])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a79ef88-7450-401a-8ea3-9c0768bd4f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd = spark.parallelize([1,2,3,4,5])\n",
    "sample_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "219e28c7-a166-4662-8622-b147d11e6e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "sample_rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "185c55b9-0120-4c89-8c9b-816c9fe78cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/spark_start/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "7b4b24e4-d013-4ade-b4f4-597de2f87e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"\n",
      "Operator Interface\n",
      "\n",
      "This module exports a set of functions corresponding to the intrinsic\n",
      "operators of Python.  For example, operator.add(x, y) is equivalent\n",
      "to the expression x+y.  The function names are those used for special\n",
      "methods; variants without leading and trailing '__' are also provided\n",
      "for convenience.\n",
      "\n",
      "This is the pure Python implementation of the module.\n",
      "\"\"\"\n",
      "\n",
      "__all__ = ['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains', 'countOf',\n",
      "           'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt', 'iadd', 'iand',\n",
      "           'iconcat', 'ifloordiv', 'ilshift', 'imatmul', 'imod', 'imul',\n",
      "           'index', 'indexOf', 'inv', 'invert', 'ior', 'ipow', 'irshift',\n",
      "           'is_', 'is_not', 'isub', 'itemgetter', 'itruediv', 'ixor', 'le',\n",
      "           'length_hint', 'lshift', 'lt', 'matmul', 'methodcaller', 'mod',\n",
      "           'mul', 'ne', 'neg', 'not_', 'or_', 'pos', 'pow', 'rshift',\n",
      "           'setitem', 'sub', 'truediv', 'truth', 'xor']\n",
      "\n",
      "from builtins import abs as _abs\n",
      "\n",
      "\n",
      "# Comparison Operations *******************************************************#\n",
      "\n",
      "def lt(a, b):\n",
      "    \"Same as a < b.\"\n",
      "    return a < b\n",
      "\n",
      "def le(a, b):\n",
      "    \"Same as a <= b.\"\n",
      "    return a <= b\n",
      "\n",
      "def eq(a, b):\n",
      "    \"Same as a == b.\"\n",
      "    return a == b\n",
      "\n",
      "def ne(a, b):\n",
      "    \"Same as a != b.\"\n",
      "    return a != b\n",
      "\n",
      "def ge(a, b):\n",
      "    \"Same as a >= b.\"\n",
      "    return a >= b\n",
      "\n",
      "def gt(a, b):\n",
      "    \"Same as a > b.\"\n",
      "    return a > b\n",
      "\n",
      "# Logical Operations **********************************************************#\n",
      "\n",
      "def not_(a):\n",
      "    \"Same as not a.\"\n",
      "    return not a\n",
      "\n",
      "def truth(a):\n",
      "    \"Return True if a is true, False otherwise.\"\n",
      "    return True if a else False\n",
      "\n",
      "def is_(a, b):\n",
      "    \"Same as a is b.\"\n",
      "    return a is b\n",
      "\n",
      "def is_not(a, b):\n",
      "    \"Same as a is not b.\"\n",
      "    return a is not b\n",
      "\n",
      "# Mathematical/Bitwise Operations *********************************************#\n",
      "\n",
      "def abs(a):\n",
      "    \"Same as abs(a).\"\n",
      "    return _abs(a)\n",
      "\n",
      "def add(a, b):\n",
      "    \"Same as a + b.\"\n",
      "    return a + b\n",
      "\n",
      "def and_(a, b):\n",
      "    \"Same as a & b.\"\n",
      "    return a & b\n",
      "\n",
      "def floordiv(a, b):\n",
      "    \"Same as a // b.\"\n",
      "    return a // b\n",
      "\n",
      "def index(a):\n",
      "    \"Same as a.__index__().\"\n",
      "    return a.__index__()\n",
      "\n",
      "def inv(a):\n",
      "    \"Same as ~a.\"\n",
      "    return ~a\n",
      "invert = inv\n",
      "\n",
      "def lshift(a, b):\n",
      "    \"Same as a << b.\"\n",
      "    return a << b\n",
      "\n",
      "def mod(a, b):\n",
      "    \"Same as a % b.\"\n",
      "    return a % b\n",
      "\n",
      "def mul(a, b):\n",
      "    \"Same as a * b.\"\n",
      "    return a * b\n",
      "\n",
      "def matmul(a, b):\n",
      "    \"Same as a @ b.\"\n",
      "    return a @ b\n",
      "\n",
      "def neg(a):\n",
      "    \"Same as -a.\"\n",
      "    return -a\n",
      "\n",
      "def or_(a, b):\n",
      "    \"Same as a | b.\"\n",
      "    return a | b\n",
      "\n",
      "def pos(a):\n",
      "    \"Same as +a.\"\n",
      "    return +a\n",
      "\n",
      "def pow(a, b):\n",
      "    \"Same as a ** b.\"\n",
      "    return a ** b\n",
      "\n",
      "def rshift(a, b):\n",
      "    \"Same as a >> b.\"\n",
      "    return a >> b\n",
      "\n",
      "def sub(a, b):\n",
      "    \"Same as a - b.\"\n",
      "    return a - b\n",
      "\n",
      "def truediv(a, b):\n",
      "    \"Same as a / b.\"\n",
      "    return a / b\n",
      "\n",
      "def xor(a, b):\n",
      "    \"Same as a ^ b.\"\n",
      "    return a ^ b\n",
      "\n",
      "# Sequence Operations *********************************************************#\n",
      "\n",
      "def concat(a, b):\n",
      "    \"Same as a + b, for a and b sequences.\"\n",
      "    if not hasattr(a, '__getitem__'):\n",
      "        msg = \"'%s' object can't be concatenated\" % type(a).__name__\n",
      "        raise TypeError(msg)\n",
      "    return a + b\n",
      "\n",
      "def contains(a, b):\n",
      "    \"Same as b in a (note reversed operands).\"\n",
      "    return b in a\n",
      "\n",
      "def countOf(a, b):\n",
      "    \"Return the number of times b occurs in a.\"\n",
      "    count = 0\n",
      "    for i in a:\n",
      "        if i == b:\n",
      "            count += 1\n",
      "    return count\n",
      "\n",
      "def delitem(a, b):\n",
      "    \"Same as del a[b].\"\n",
      "    del a[b]\n",
      "\n",
      "def getitem(a, b):\n",
      "    \"Same as a[b].\"\n",
      "    return a[b]\n",
      "\n",
      "def indexOf(a, b):\n",
      "    \"Return the first index of b in a.\"\n",
      "    for i, j in enumerate(a):\n",
      "        if j == b:\n",
      "            return i\n",
      "    else:\n",
      "        raise ValueError('sequence.index(x): x not in sequence')\n",
      "\n",
      "def setitem(a, b, c):\n",
      "    \"Same as a[b] = c.\"\n",
      "    a[b] = c\n",
      "\n",
      "def length_hint(obj, default=0):\n",
      "    \"\"\"\n",
      "    Return an estimate of the number of items in obj.\n",
      "    This is useful for presizing containers when building from an iterable.\n",
      "\n",
      "    If the object supports len(), the result will be exact. Otherwise, it may\n",
      "    over- or under-estimate by an arbitrary amount. The result will be an\n",
      "    integer >= 0.\n",
      "    \"\"\"\n",
      "    if not isinstance(default, int):\n",
      "        msg = (\"'%s' object cannot be interpreted as an integer\" %\n",
      "               type(default).__name__)\n",
      "        raise TypeError(msg)\n",
      "\n",
      "    try:\n",
      "        return len(obj)\n",
      "    except TypeError:\n",
      "        pass\n",
      "\n",
      "    try:\n",
      "        hint = type(obj).__length_hint__\n",
      "    except AttributeError:\n",
      "        return default\n",
      "\n",
      "    try:\n",
      "        val = hint(obj)\n",
      "    except TypeError:\n",
      "        return default\n",
      "    if val is NotImplemented:\n",
      "        return default\n",
      "    if not isinstance(val, int):\n",
      "        msg = ('__length_hint__ must be integer, not %s' %\n",
      "               type(val).__name__)\n",
      "        raise TypeError(msg)\n",
      "    if val < 0:\n",
      "        msg = '__length_hint__() should return >= 0'\n",
      "        raise ValueError(msg)\n",
      "    return val\n",
      "\n",
      "# Generalized Lookup Objects **************************************************#\n",
      "\n",
      "class attrgetter:\n",
      "    \"\"\"\n",
      "    Return a callable object that fetches the given attribute(s) from its operand.\n",
      "    After f = attrgetter('name'), the call f(r) returns r.name.\n",
      "    After g = attrgetter('name', 'date'), the call g(r) returns (r.name, r.date).\n",
      "    After h = attrgetter('name.first', 'name.last'), the call h(r) returns\n",
      "    (r.name.first, r.name.last).\n",
      "    \"\"\"\n",
      "    __slots__ = ('_attrs', '_call')\n",
      "\n",
      "    def __init__(self, attr, *attrs):\n",
      "        if not attrs:\n",
      "            if not isinstance(attr, str):\n",
      "                raise TypeError('attribute name must be a string')\n",
      "            self._attrs = (attr,)\n",
      "            names = attr.split('.')\n",
      "            def func(obj):\n",
      "                for name in names:\n",
      "                    obj = getattr(obj, name)\n",
      "                return obj\n",
      "            self._call = func\n",
      "        else:\n",
      "            self._attrs = (attr,) + attrs\n",
      "            getters = tuple(map(attrgetter, self._attrs))\n",
      "            def func(obj):\n",
      "                return tuple(getter(obj) for getter in getters)\n",
      "            self._call = func\n",
      "\n",
      "    def __call__(self, obj):\n",
      "        return self._call(obj)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
      "                              self.__class__.__qualname__,\n",
      "                              ', '.join(map(repr, self._attrs)))\n",
      "\n",
      "    def __reduce__(self):\n",
      "        return self.__class__, self._attrs\n",
      "\n",
      "class itemgetter:\n",
      "    \"\"\"\n",
      "    Return a callable object that fetches the given item(s) from its operand.\n",
      "    After f = itemgetter(2), the call f(r) returns r[2].\n",
      "    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
      "    \"\"\"\n",
      "    __slots__ = ('_items', '_call')\n",
      "\n",
      "    def __init__(self, item, *items):\n",
      "        if not items:\n",
      "            self._items = (item,)\n",
      "            def func(obj):\n",
      "                return obj[item]\n",
      "            self._call = func\n",
      "        else:\n",
      "            self._items = items = (item,) + items\n",
      "            def func(obj):\n",
      "                return tuple(obj[i] for i in items)\n",
      "            self._call = func\n",
      "\n",
      "    def __call__(self, obj):\n",
      "        return self._call(obj)\n",
      "\n",
      "    def __repr__(self):\n",
      "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
      "                              self.__class__.__name__,\n",
      "                              ', '.join(map(repr, self._items)))\n",
      "\n",
      "    def __reduce__(self):\n",
      "        return self.__class__, self._items\n",
      "\n",
      "class methodcaller:\n",
      "    \"\"\"\n",
      "    Return a callable object that calls the given method on its operand.\n",
      "    After f = methodcaller('name'), the call f(r) returns r.name().\n",
      "    After g = methodcaller('name', 'date', foo=1), the call g(r) returns\n",
      "    r.name('date', foo=1).\n",
      "    \"\"\"\n",
      "    __slots__ = ('_name', '_args', '_kwargs')\n",
      "\n",
      "    def __init__(self, name, /, *args, **kwargs):\n",
      "        self._name = name\n",
      "        if not isinstance(self._name, str):\n",
      "            raise TypeError('method name must be a string')\n",
      "        self._args = args\n",
      "        self._kwargs = kwargs\n",
      "\n",
      "    def __call__(self, obj):\n",
      "        return getattr(obj, self._name)(*self._args, **self._kwargs)\n",
      "\n",
      "    def __repr__(self):\n",
      "        args = [repr(self._name)]\n",
      "        args.extend(map(repr, self._args))\n",
      "        args.extend('%s=%r' % (k, v) for k, v in self._kwargs.items())\n",
      "        return '%s.%s(%s)' % (self.__class__.__module__,\n",
      "                              self.__class__.__name__,\n",
      "                              ', '.join(args))\n",
      "\n",
      "    def __reduce__(self):\n",
      "        if not self._kwargs:\n",
      "            return self.__class__, (self._name,) + self._args\n",
      "        else:\n",
      "            from functools import partial\n",
      "            return partial(self.__class__, self._name, **self._kwargs), self._args\n",
      "\n",
      "\n",
      "# In-place Operations *********************************************************#\n",
      "\n",
      "def iadd(a, b):\n",
      "    \"Same as a += b.\"\n",
      "    a += b\n",
      "    return a\n",
      "\n",
      "def iand(a, b):\n",
      "    \"Same as a &= b.\"\n",
      "    a &= b\n",
      "    return a\n",
      "\n",
      "def iconcat(a, b):\n",
      "    \"Same as a += b, for a and b sequences.\"\n",
      "    if not hasattr(a, '__getitem__'):\n",
      "        msg = \"'%s' object can't be concatenated\" % type(a).__name__\n",
      "        raise TypeError(msg)\n",
      "    a += b\n",
      "    return a\n",
      "\n",
      "def ifloordiv(a, b):\n",
      "    \"Same as a //= b.\"\n",
      "    a //= b\n",
      "    return a\n",
      "\n",
      "def ilshift(a, b):\n",
      "    \"Same as a <<= b.\"\n",
      "    a <<= b\n",
      "    return a\n",
      "\n",
      "def imod(a, b):\n",
      "    \"Same as a %= b.\"\n",
      "    a %= b\n",
      "    return a\n",
      "\n",
      "def imul(a, b):\n",
      "    \"Same as a *= b.\"\n",
      "    a *= b\n",
      "    return a\n",
      "\n",
      "def imatmul(a, b):\n",
      "    \"Same as a @= b.\"\n",
      "    a @= b\n",
      "    return a\n",
      "\n",
      "def ior(a, b):\n",
      "    \"Same as a |= b.\"\n",
      "    a |= b\n",
      "    return a\n",
      "\n",
      "def ipow(a, b):\n",
      "    \"Same as a **= b.\"\n",
      "    a **=b\n",
      "    return a\n",
      "\n",
      "def irshift(a, b):\n",
      "    \"Same as a >>= b.\"\n",
      "    a >>= b\n",
      "    return a\n",
      "\n",
      "def isub(a, b):\n",
      "    \"Same as a -= b.\"\n",
      "    a -= b\n",
      "    return a\n",
      "\n",
      "def itruediv(a, b):\n",
      "    \"Same as a /= b.\"\n",
      "    a /= b\n",
      "    return a\n",
      "\n",
      "def ixor(a, b):\n",
      "    \"Same as a ^= b.\"\n",
      "    a ^= b\n",
      "    return a\n",
      "\n",
      "\n",
      "try:\n",
      "    from _operator import *\n",
      "except ImportError:\n",
      "    pass\n",
      "else:\n",
      "    from _operator import __doc__\n",
      "\n",
      "# All of these \"__func__ = func\" assignments have to happen after importing\n",
      "# from _operator to make sure they're set to the right function\n",
      "__lt__ = lt\n",
      "__le__ = le\n",
      "__eq__ = eq\n",
      "__ne__ = ne\n",
      "__ge__ = ge\n",
      "__gt__ = gt\n",
      "__not__ = not_\n",
      "__abs__ = abs\n",
      "__add__ = add\n",
      "__and__ = and_\n",
      "__floordiv__ = floordiv\n",
      "__index__ = index\n",
      "__inv__ = inv\n",
      "__invert__ = invert\n",
      "__lshift__ = lshift\n",
      "__mod__ = mod\n",
      "__mul__ = mul\n",
      "__matmul__ = matmul\n",
      "__neg__ = neg\n",
      "__or__ = or_\n",
      "__pos__ = pos\n",
      "__pow__ = pow\n",
      "__rshift__ = rshift\n",
      "__sub__ = sub\n",
      "__truediv__ = truediv\n",
      "__xor__ = xor\n",
      "__concat__ = concat\n",
      "__contains__ = contains\n",
      "__delitem__ = delitem\n",
      "__getitem__ = getitem\n",
      "__setitem__ = setitem\n",
      "__iadd__ = iadd\n",
      "__iand__ = iand\n",
      "__iconcat__ = iconcat\n",
      "__ifloordiv__ = ifloordiv\n",
      "__ilshift__ = ilshift\n",
      "__imod__ = imod\n",
      "__imul__ = imul\n",
      "__imatmul__ = imatmul\n",
      "__ior__ = ior\n",
      "__ipow__ = ipow\n",
      "__irshift__ = irshift\n",
      "__isub__ = isub\n",
      "__itruediv__ = itruediv\n",
      "__ixor__ = ixor\n"
     ]
    }
   ],
   "source": [
    "!cat /home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/operator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b572f0-70c3-41c2-b673-38dade0e8acb",
   "metadata": {},
   "source": [
    "### partition이 나뉘어져 있을때\n",
    "#### 연산식 예제\n",
    "- lambda x, y : (x*2) + y\n",
    "1) (x,y) = (1,2) >> 1*2 + 2 = 4\n",
    "2) (x,y) = (4,3) >> 4*2 + 3 = 11\n",
    "3) (x,y) = (11,4) >> 22 + 4 = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "008233dc-d640-47e6-9969-330a5622fef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce는 앞 두 요소를 가져와 압축하는 느낌\n",
    "sample_rdd1 = spark.parallelize([1,2,3,4])\n",
    "sample_rdd1.reduce(lambda x, y : (x * 2 + y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f1292e2-8716-428a-ae74-6011ec4360f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "685f76e5-9728-4873-bac9-067df3518c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd2 = spark.parallelize([1,2,3,4], 2)\n",
    "\n",
    "sample_rdd2.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1f1f31b-37f4-48f7-84e5-322223af7efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd2.reduce(lambda x,y : (x*2 + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c556ce3-492f-4799-8b24-3c4deee1aa41",
   "metadata": {},
   "source": [
    "###### 파티션이 2개일떄 계산 방식\n",
    "- (x, y) = (1,2) >> 4 = a\n",
    "- (x,y) = (3,4) >> 10 = b\n",
    "- (x,y) = (a,b) >> 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "afdf5874-2632-4e8e-904b-e7eebcb64e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1], [2], [3, 4]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파티션이 3개일때\n",
    "sample_rdd3 = spark.parallelize([1,2,3,4], 3)\n",
    "sample_rdd3.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63b7cb55-d1d0-4759-bd26-1f58964e6709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_rdd3.reduce(lambda x, y : (x*2 + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f787bc7-fe71-4bfd-aaa2-642eaaea52c2",
   "metadata": {},
   "source": [
    "###### 파티션이 3개일때 계산 방식\n",
    "- parition1  \n",
    "요소가 하나라서 계산하지 않음\n",
    "\n",
    "- parition2  \n",
    "요소가 하나라서 계산하지 않음\n",
    "\n",
    "- partition3\n",
    "(x,y) = (3,4) >> 10\n",
    "\n",
    "- partition1 & partition2\n",
    "(x,y) = (1,2) >> 4\n",
    "\n",
    "- partition1 & partition2 + partition3\n",
    "\n",
    "(x,y) = (4,10) >> 18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf44b10-6bc0-42d4-a730-236f9c577105",
   "metadata": {},
   "source": [
    "### 결론 : reduce 연산은 순서 의존적인 방식이다.\n",
    "- 각 파티션 내에서, 연산은 독립적으로 실행, 최종 결과는 파티션 간의 결과가 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b17ff9-1214-46da-869a-bbd32710f171",
   "metadata": {},
   "source": [
    "### fold 연산의 활용\n",
    "fold(zeroValue, [<함수>])\n",
    "\n",
    "zeroValue : 시작값, 항등원 *일 때 1, +일 경우 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f9dd3763-cb54-4ed7-a8ff-48f34b534424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [2], [3], [4]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = spark.parallelize([2,3,4], 4)\n",
    "rdd4.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b69be363-a7dd-4759-b926-1401fd1d96e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.reduce(lambda x, y : x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0409d0d1-6f0e-40c6-aa82-b6b800e507b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.fold(1, lambda x, y : x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "953b287e-bb42-4f34-86e9-5f8a354e88fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.fold(2, lambda x, y : x * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fb50bb89-0029-4cea-827a-0216145b3784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce 에러납니다 : Can not reduce() empty RDD\n"
     ]
    }
   ],
   "source": [
    "# 빈 RDD의 예외처리\n",
    "rdd5 = spark.parallelize([])\n",
    "try :\n",
    "    rdd5.reduce(lambda a, b : a+b)\n",
    "\n",
    "except ValueError as e :\n",
    "    print(f'Reduce 에러납니다 : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6fd7d133-3354-4397-b163-f2eb526defe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd5.fold(0, lambda a, b : a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9573031-3d89-4d9f-9b07-87cb16528f0c",
   "metadata": {},
   "source": [
    "### reduce(), fold() 비슷한 처리를 합니다.\n",
    "1. reduce()는 단순한 RDD 축약 연산 >> 비어있지 않을때, 사용\n",
    "     \n",
    "2. fold()는 일반적이고, 오류가 없는 RDD 축약 연산 >> 초기값(zeroValue)을 적용해서, 비어있는 RDD도 계산 해줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "005ecf6f-e8a6-4c15-90c7-4be34382a946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 'a', 2]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6 = spark.parallelize([1,'a', 2])\n",
    "rdd6.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0f13a725-5a34-4038-8bd0-fc64873974d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce 에러가 나네요\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 11:54:15 ERROR Executor: Exception in task 0.0 in stage 50.0 (TID 77)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5990/3825952491.py\", line 3, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/05 11:54:15 WARN TaskSetManager: Lost task 0.0 in stage 50.0 (TID 77) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5990/3825952491.py\", line 3, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/12/05 11:54:15 ERROR TaskSetManager: Task 0 in stage 50.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "try :\n",
    "    res = rdd6.reduce(lambda x,y : x+y)\n",
    "    print(type(res), res)\n",
    "    \n",
    "except Py4JJavaError as e:\n",
    "    print(f'reduce 에러가 나네요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b15be8a-4463-4f25-b5f8-4e62b35ff5ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduce 에러가 나네요 : An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 66) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5990/1627123937.py\", line 3, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.GeneratedMethodAccessor40.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5990/1627123937.py\", line 3, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/05 11:50:55 ERROR Executor: Exception in task 0.0 in stage 39.0 (TID 66)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5990/1627123937.py\", line 3, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/12/05 11:50:55 WARN TaskSetManager: Lost task 0.0 in stage 39.0 (TID 66) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 259, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/opt/spark/python/pyspark/rdd.py\", line 995, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "  File \"/opt/spark/python/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_5990/1627123937.py\", line 3, in <lambda>\n",
      "TypeError: unsupported operand type(s) for +: 'int' and 'str'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "24/12/05 11:50:55 ERROR TaskSetManager: Task 0 in stage 39.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "from py4j.protocol import Py4JJavaError\n",
    "try :\n",
    "    res = rdd6.reduce(lambda x,y : str(x)+str(y))\n",
    "    print(type(res), res)\n",
    "    \n",
    "except Py4JJavaError as e:\n",
    "    print(f'reduce 에러가 나네요 : {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c824373d-a6ad-473a-afd0-da1eedd16c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1a2'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fold는 parameter(zeroValue)를 지정할 수 있다\n",
    "rdd6.fold(\"\", lambda x, y : str(x)+ str(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f3f26d2-e144-4b79-b683-76ff624b0b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a * b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a8ebf20-8c7e-4717-8f43-ff027747e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 2, 3, 1, 1, 7, 4, 2, 25, 1, 6, 7, 1, 2, 3]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupBy(), aggregate()\n",
    "rdd7 = spark.parallelize([1,2,3,1,2,3,1,1,7,4,2,25,1,6,7,1,2,3])\n",
    "rdd7.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1bfd08d5-d195-4cdc-81b0-95b620ba75bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x7f888584e4f0>),\n",
       " (0, <pyspark.resultiterable.ResultIterable at 0x7f888584e910>)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd7.groupBy(lambda x : x%2).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd69b69b-3840-4131-aaaa-66a3104308f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [1, 1, 1, 1, 1, 1, 3, 3, 3, 7, 7, 25]\n",
      "0 [2, 2, 2, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "for i, j in rdd7.groupBy(lambda x : x%2).collect():\n",
    "    print(i, sorted(list(j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8dfa3262-8828-48dc-87b4-2df6a6d93123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [2, 2, 2, 2, 4, 6]), (1, [1, 1, 1, 1, 1, 1, 3, 3, 3, 7, 7, 25])]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(x, sorted(y)) for (x,y) in rdd7.groupBy(lambda x : x%2).collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c2bf6-d2bb-4d64-881a-21bfae32f146",
   "metadata": {},
   "source": [
    "#### aggregate\n",
    "RDD.aggregate(zeroValue, func1(partition 내부의 규칙), fun2(fun2_combine))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "264525d1-9da0-485b-8b3b-4d6defac4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (합계 , 개수)\n",
    "func1 = lambda x, y : (x[0] + y, x[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "bfbf053d-2e19-4d8f-bf6a-8012e656bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (합계 , 개수)\n",
    "func2 = lambda x, y : (x[0] + y[0], x[1] + y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cc5933ab-cfa9-4ffe-a38c-3fe08c94e271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8 = spark.parallelize([1,2,3,4], 2)\n",
    "rdd8.glom().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0240807f-2bfc-40bf-9492-f31b58f3eed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 4)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd8.aggregate((0,0),func1, func2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a7662-82d2-42c9-88bf-cfb7cd70633f",
   "metadata": {},
   "source": [
    "### partition1\n",
    "x=[1,2], zeroValule = [0,0]  \n",
    "x[0] = 0, x[1] = 0  \n",
    "x[0] + y = 0 + 1, x[1]+1 = 0+1 = 1 >> (1,1)  \n",
    "x[0] + y = 1 + 2, x[1]+1 = 1+1 = 2 >> (3,2)  \n",
    "\n",
    "### partition2  \n",
    "x=[3,4], zeroValue[0,0]    \n",
    "x[0]=0, x[1] = 0  \n",
    "x[0]+y = 0+3 = 3, x[1] + y = 0+1=1 >> (3,1)  \n",
    "x[0]+y = 3+4 = 7, x[1] + y = 1+1=2 >> (7,2)  \n",
    "  \n",
    "### partition1 + partition2  \n",
    "(x, y) = ((3,2), (7,2))  \n",
    "x[0] + y[0] = 3 + 7= 10, x[1] + y[1] 2 + 2 = 4 >> (10,4)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b76c3-097f-4451-ae68-9971fe98398e",
   "metadata": {},
   "source": [
    "### key-value RDD(Paired RDD)\n",
    "  \n",
    "#### groupByKey(), reduceByKey()  \n",
    "  \n",
    "그룹핑 한 후에 특정 트랜스포메이션을 수행함. 키가 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "892e94dd-3d1f-4902-af59-5bee3f944e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd9 = spark.parallelize(\n",
    "    [\n",
    "        ('짜장면', 15),\n",
    "        ('짬뽕', 10),\n",
    "        ('짜장면', 5),\n",
    "        ('짬뽕', 20),\n",
    "        ('짜장면', 1),\n",
    "        ('짬뽕', 2),\n",
    "        ('짜장면', 3),\n",
    "        ('짬뽕', 4),\n",
    "    ]   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8d0f4429-bce0-4ddb-9389-893b7e271492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('짜장면', <pyspark.resultiterable.ResultIterable object at 0x7f8885725af0>), ('짬뽕', <pyspark.resultiterable.ResultIterable object at 0x7f888577e160>)]\n",
      "[('짜장면', 4), ('짬뽕', 4)]\n",
      "[('짜장면', 15), ('짬뽕', 20)]\n",
      "[('짜장면', 1), ('짬뽕', 2)]\n",
      "[('짜장면', [1, 3, 5, 15]), ('짬뽕', [2, 4, 10, 20])]\n",
      "[('짜장면', [15, 5, 1, 3]), ('짬뽕', [10, 20, 2, 4])]\n"
     ]
    }
   ],
   "source": [
    "rdd9_groupby = rdd9.groupByKey()\n",
    "print(rdd9_groupby.mapValues(lambda x: x).collect())\n",
    "print(rdd9_groupby.mapValues(len).collect())\n",
    "print(rdd9_groupby.mapValues(max).collect())\n",
    "print(rdd9_groupby.mapValues(min).collect())\n",
    "print(rdd9_groupby.mapValues(sorted).collect())\n",
    "print(rdd9_groupby.mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cdd77cd0-8904-49f1-a9b7-afe2ad4798f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('짜장면', 6.0), ('짬뽕', 9.0)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9_groupby.mapValues(lambda x : sum(x) / len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "82d9e43f-ae8d-4761-b8b5-80534b3a4a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby 활용 : [('짜장면', 24), ('짬뽕', 36)]\n",
      "reduceByKey 활용 : [('짜장면', 24), ('짬뽕', 36)]\n",
      "reduceByKey + lambda 활용 : [('짜장면', 24), ('짬뽕', 36)]\n",
      "***groupByKey보다는 reduceByKey가 성능상 빠름***\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "print(f'groupby 활용 : {rdd9.groupByKey().mapValues(lambda x : sum(x)).collect()}')\n",
    "print(f'reduceByKey 활용 : {rdd9.reduceByKey(add).collect()}')\n",
    "print(f'reduceByKey + lambda 활용 : {rdd9.reduceByKey(lambda x, y: x+y).collect()}')\n",
    "print('***groupByKey보다는 reduceByKey가 성능상 빠름***')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e528a15-e3f4-4b29-9ad3-07f06733d72e",
   "metadata": {},
   "source": [
    "문제 원인:\n",
    "\n",
    "reduceByKey(lambda x: x + x)는 두 값을 병합하는 함수가 아니라, 하나의 값을 단순히 두 배로 만드는 함수이므로 요구사항에 맞지 않음.\n",
    "수정 방법:\n",
    "\n",
    "두 값을 병합하는 올바른 함수(lambda x, y: x + y)를 제공해야 함.\n",
    "값 변환이 필요하다면, mapValues와 같은 메서드를 추가로 사용.\n",
    "결합 법칙 중요성:\n",
    "\n",
    "reduceByKey의 함수는 결합 법칙을 만족해야 하며, Spark는 파티션별로 데이터를 병합한 후 전체적으로 결합합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca300db5-e64d-4826-a58d-c4aed9f7f5d8",
   "metadata": {},
   "source": [
    "#### countByKey()\n",
    "각 키별로 요소들의 갯수를 카운트 > Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "6c424f42-6a55-4b92-9d90-0073788e2fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'짜장면': 4, '짬뽕': 4})"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fe1cd143-179a-48d5-bca9-e82c7a892ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {('짜장면', 15): 1,\n",
       "             ('짬뽕', 10): 1,\n",
       "             ('짜장면', 5): 1,\n",
       "             ('짬뽕', 20): 1,\n",
       "             ('짜장면', 1): 1,\n",
       "             ('짬뽕', 2): 1,\n",
       "             ('짜장면', 3): 1,\n",
       "             ('짬뽕', 4): 1})"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9914b18b-6704-4af7-b453-7b1274cfca22",
   "metadata": {},
   "source": [
    "#### keys() 키만 가진 RDD 생성 >> Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "2428d147-54bb-4eb6-9130-5cb9a511029f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['짜장면', '짬뽕', '짜장면', '짬뽕', '짜장면', '짬뽕', '짜장면', '짬뽕']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd9.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d19681f-8e66-4593-821a-9917fc6bfbe8",
   "metadata": {},
   "source": [
    "#### join(outer, inner) : Key를 기준으로 join함(집합연산{intersection, subtract, union}이랑은 결과가 다름)\n",
    "- inner join(default)  \n",
    "2개의 집합에서 교집합  \n",
    "=> 두 개의 집합에서 함께 존재하는 요소(key)의 집합  \n",
    "   \n",
    "- outer join    \n",
    "2개의 집합에서 합집합  \n",
    "=> 한쪽에 있고, 다른쪽에 없을때(left, right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ad2dbcf5-03b9-4a47-b9b9-3adf5a24db88",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_rdd1 = spark.parallelize(\n",
    "    [\n",
    "    ('a', 10),\n",
    "    ('b', 20),\n",
    "    ('c', 30)\n",
    "    ]\n",
    ")\n",
    "\n",
    "join_rdd2 = spark.parallelize(\n",
    "    [\n",
    "    ('a', 20),\n",
    "    ('b', 20),\n",
    "    ('b', 10),\n",
    "    ('d', 5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9164f276-8766-49f2-b7bc-4bd5f99de2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 20)), ('b', (20, 10)), ('a', (10, 20))]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1.join(join_rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "949c4fbe-d754-413f-94c1-a120235f6dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 20)]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1.intersection(join_rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "64b8ae87-54de-40af-b6a4-57049bb29a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 20)), ('b', (20, 10)), ('c', (30, None)), ('a', (10, 20))]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1.leftOuterJoin(join_rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2f249721-42da-4435-8fe7-c337c696e340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', (20, 20)), ('b', (20, 10)), ('d', (None, 5)), ('a', (10, 20))]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_rdd1.rightOuterJoin(join_rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b8d23723-577c-4f43-b87e-80a8889785b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc519437-f976-4cf6-94d8-6aa8c0450b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
