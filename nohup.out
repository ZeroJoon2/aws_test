[I 2024-12-06 10:09:40.139 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2024-12-06 10:09:40.143 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2024-12-06 10:09:40.148 ServerApp] jupyterlab | extension was successfully linked.
[W 2024-12-06 10:09:40.149 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2024-12-06 10:09:40.152 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2024-12-06 10:09:40.152 ServerApp] notebook | extension was successfully linked.
[I 2024-12-06 10:09:40.630 ServerApp] notebook_shim | extension was successfully linked.
[I 2024-12-06 10:09:40.739 ServerApp] notebook_shim | extension was successfully loaded.
[I 2024-12-06 10:09:40.742 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2024-12-06 10:09:40.743 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2024-12-06 10:09:40.755 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab
[I 2024-12-06 10:09:40.755 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/spark_start/share/jupyter/lab
[I 2024-12-06 10:09:40.755 LabApp] Extension Manager is 'pypi'.
[W 2024-12-06 10:09:40.755 LabApp] Failed to instantiate the extension manager pypi. Falling back to read-only manager.
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/labapp.py", line 837, in initialize_handlers
        ext_manager = manager_factory(app_options, listings_config, self)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/extensions/__init__.py", line 46, in get_pypi_manager
        return PyPIExtensionManager(app_options, ext_options, parent)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/extensions/pypi.py", line 134, in __init__
        self._httpx_client = httpx.AsyncClient(proxies=proxies)
    TypeError: __init__() got an unexpected keyword argument 'proxies'
[I 2024-12-06 10:09:40.763 ServerApp] jupyterlab | extension was successfully loaded.
[I 2024-12-06 10:09:40.767 ServerApp] notebook | extension was successfully loaded.
[I 2024-12-06 10:09:40.767 ServerApp] Serving notebooks from local directory: /home/lab17/git
[I 2024-12-06 10:09:40.767 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2024-12-06 10:09:40.767 ServerApp] http://ip-172-31-3-9:8917/tree
[I 2024-12-06 10:09:40.767 ServerApp]     http://127.0.0.1:8917/tree
[I 2024-12-06 10:09:40.767 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2024-12-06 10:09:40.785 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2024-12-06 10:09:50.711 ServerApp] 302 GET / (@125.129.250.60) 0.43ms
[I 2024-12-06 10:09:50.740 JupyterNotebookApp] 302 GET /tree? (@125.129.250.60) 0.50ms
[I 2024-12-06 10:09:55.860 ServerApp] User 169f9bb23d6b4cba84ef4060bf086c06 logged in.
[I 2024-12-06 10:09:55.861 ServerApp] 302 POST /login?next=%2Ftree%3F (169f9bb23d6b4cba84ef4060bf086c06@125.129.250.60) 81.83ms
[I 2024-12-06 11:32:11.777 ServerApp] Creating new notebook in /src
[I 2024-12-06 11:32:12.196 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 11:32:12.314 ServerApp] Kernel started: 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b
[I 2024-12-06 11:32:12.947 ServerApp] Connecting to kernel 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b.
[I 2024-12-06 11:32:13.036 ServerApp] Connecting to kernel 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b.
[I 2024-12-06 11:32:13.206 ServerApp] Connecting to kernel 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b.
[I 2024-12-06 11:32:13.551 ServerApp] Connecting to kernel 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b.
[I 2024-12-06 11:34:13.505 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 11:34:50.042 ServerApp] Connecting to kernel 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b.
[I 2024-12-06 11:34:50.585 ServerApp] Kernel started: fe065a22-d9b1-4903-a0ec-029017bb7b77
[I 2024-12-06 11:34:51.025 ServerApp] Connecting to kernel fe065a22-d9b1-4903-a0ec-029017bb7b77.
24/12/06 11:35:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2024-12-06 11:35:36.065 ServerApp] Kernel interrupted: 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b
[I 2024-12-06 11:35:40.920 ServerApp] Kernel shutdown: fe065a22-d9b1-4903-a0ec-029017bb7b77
[I 2024-12-06 11:35:40.922 ServerApp] Kernel shutdown: 9cce8cf2-6a7e-4bd2-8ad2-7025a986dd7b
[I 2024-12-06 11:35:47.383 ServerApp] Kernel started: 6062bbe8-0914-4ff4-b6c2-84e15218c129
[I 2024-12-06 11:35:47.881 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 11:35:47.945 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 11:35:48.013 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 11:35:50.745 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
24/12/06 11:36:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2024-12-06 11:36:07.500 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:38:07.604 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2024-12-06 11:38:35.478 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:38:58.417 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:40:02.402 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:40:58.534 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:43:20.183 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:46:24.302 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 11:48:24.901 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
24/12/06 13:10:22 ERROR Executor: Exception in task 0.0 in stage 7.0 (TID 7)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 259, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/pyspark/rdd.py", line 992, in func
    initial = next(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 1448, in countPartition
    counts[obj] += 1
TypeError: unhashable type: 'list'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/12/06 13:10:22 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 259, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/pyspark/rdd.py", line 992, in func
    initial = next(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 1448, in countPartition
    counts[obj] += 1
TypeError: unhashable type: 'list'

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

24/12/06 13:10:22 ERROR TaskSetManager: Task 0 in stage 7.0 failed 1 times; aborting job
24/12/06 13:10:34 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 259, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/pyspark/rdd.py", line 1560, in takeUpToNumLeft
    yield next(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/python/pyspark/rdd.py", line 1871, in <lambda>
    return self.map(lambda x: x[1])
IndexError: list index out of range

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/12/06 13:10:34 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 259, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/pyspark/rdd.py", line 1560, in takeUpToNumLeft
    yield next(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/python/pyspark/rdd.py", line 1871, in <lambda>
    return self.map(lambda x: x[1])
IndexError: list index out of range

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

24/12/06 13:10:34 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job
[I 2024-12-06 13:10:57.811 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
24/12/06 13:11:29 ERROR Executor: Exception in task 0.0 in stage 10.0 (TID 10)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 259, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/pyspark/rdd.py", line 992, in func
    initial = next(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 1447, in countPartition
    for obj in iterator:
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/python/pyspark/rdd.py", line 1933, in <lambda>
    return self.map(lambda x: x[0]).countByValue()
IndexError: list index out of range

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/12/06 13:11:29 WARN TaskSetManager: Lost task 0.0 in stage 10.0 (TID 10) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 596, in process
    serializer.dump_stream(out_iter, outfile)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py", line 259, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/opt/spark/python/pyspark/rdd.py", line 992, in func
    initial = next(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 1447, in countPartition
    for obj in iterator:
  File "/opt/spark/python/lib/pyspark.zip/pyspark/util.py", line 73, in wrapper
    return f(*args, **kwargs)
  File "/opt/spark/python/pyspark/rdd.py", line 1933, in <lambda>
    return self.map(lambda x: x[0]).countByValue()
IndexError: list index out of range

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)
	at scala.collection.TraversableOnce.to(TraversableOnce.scala:315)
	at scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)
	at scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)
	at scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

24/12/06 13:11:29 ERROR TaskSetManager: Task 0 in stage 10.0 failed 1 times; aborting job
[I 2024-12-06 13:12:57.921 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:14:15.113 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:16:15.799 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:16:36.698 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:18:36.825 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:20:36.963 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:21:49.610 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:23:49.732 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:25:49.932 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:27:50.099 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:29:50.248 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:31:50.476 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:33:31.394 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:33:38.726 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 13:34:18.492 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 13:34:19.161 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:8fdc4e9d-fa31-424a-8c9d-b277e19bf1d7
[I 2024-12-06 13:34:22.289 ServerApp] Creating new notebook in /src
[I 2024-12-06 13:34:22.492 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 13:34:22.542 ServerApp] Kernel started: 40d08d52-c5da-4d7f-a7b5-45ef71c060ea
[I 2024-12-06 13:34:23.123 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 13:34:23.351 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 13:34:23.461 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 13:34:23.610 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:be9b8f4f-97cd-42b9-9a8c-560bc008aa5f
[I 2024-12-06 13:34:23.748 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 13:36:23.788 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 13:41:01.107 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:41:03.754 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:42:02.871 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
24/12/06 13:43:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[I 2024-12-06 13:44:02.990 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:45:44.004 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:46:07.729 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2024-12-06 13:48:07.851 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:49:19.911 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:51:20.038 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[Stage 1:>                                                          (0 + 1) / 1]                                                                                [I 2024-12-06 13:53:20.164 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 13:55:20.272 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:05:09.228 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 14:05:09.302 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 14:05:09.457 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:ec434086-005d-4998-970f-9dd6b5029a27
[I 2024-12-06 14:07:22.752 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:09:22.872 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:11:22.988 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
24/12/06 14:12:18 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:197)
	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:169)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:253)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:231)
	at org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:226)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:226)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:145)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)
	at org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)
	at org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
[I 2024-12-06 14:13:23.122 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
24/12/06 14:13:50 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBlocks$1(TorrentBroadcast.scala:197)
	at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.broadcast.TorrentBroadcast.readBlocks(TorrentBroadcast.scala:169)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$4(TorrentBroadcast.scala:253)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$2(TorrentBroadcast.scala:231)
	at org.apache.spark.util.KeyLock.withLock(KeyLock.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:226)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1405)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:226)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:103)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:145)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)
	at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)
	at org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)
	at org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)
	at org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
[I 2024-12-06 14:15:23.747 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:16:21.475 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:16:53.912 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:18:54.026 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:20:54.141 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:22:54.260 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:24:12.989 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:26:13.133 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:28:13.577 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:30:13.707 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:32:13.830 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:34:14.719 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:36:14.838 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:36:24.227 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:37:29.811 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:39:30.725 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:40:44.490 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:8d753a17-5e70-42ad-b8a1-134863f5024a
[I 2024-12-06 14:41:12.776 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 14:41:12.861 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 14:41:13.071 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:1ffce499-0907-424b-8f7c-3e383be4b37e
[I 2024-12-06 14:41:13.072 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:46172a49-a931-499d-a10e-bcd59bb3a12a
[I 2024-12-06 14:41:13.419 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 14:41:17.911 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 14:41:17.990 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 14:41:18.011 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:1cc5c87d-554e-480c-baef-aeefb8626126
[I 2024-12-06 14:41:41.411 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:43:08.758 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:45:09.615 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:47:09.754 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:47:31.328 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:49:31.468 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:51:31.609 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:53:31.734 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:55:31.860 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:57:31.992 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 14:59:32.126 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:00:29.105 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:01:32.255 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:02:48.608 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:03:00.645 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:04:05.340 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:04:53.690 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:06:53.807 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:07:06.239 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:08:11.181 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:14:34.652 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:18:07.618 ServerApp] Creating new notebook in /src
[I 2024-12-06 15:18:07.836 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:18:07.917 ServerApp] Kernel started: 69f7e184-143e-4325-a3b1-cc51e30ed732
[I 2024-12-06 15:18:08.501 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 15:18:08.831 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 15:18:08.932 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 15:18:09.012 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 15:18:09.092 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:6dc96906-36dd-4ae3-86af-1b924224f3ab
[I 2024-12-06 15:18:09.187 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
24/12/06 15:18:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
24/12/06 15:18:45 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[Stage 0:>                                                          (0 + 1) / 1]                                                                                [I 2024-12-06 15:20:09.129 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:20:31.597 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:22:32.669 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:24:32.791 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:26:33.676 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:28:34.666 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-06 15:34:31.656 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:34:40.127 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:34:45.371 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 15:34:45.446 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 15:34:45.516 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 15:34:45.535 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:0c401b53-7126-48f4-abf5-1a079be86b68
[Stage 32:================>                                      (59 + 2) / 200][Stage 32:======================>                                (80 + 2) / 200][Stage 32:===========================>                          (102 + 2) / 200][Stage 32:==================================>                   (128 + 2) / 200][Stage 32:==========================================>           (158 + 2) / 200][Stage 32:===================================================>  (191 + 2) / 200]                                                                                [I 2024-12-06 15:36:40.252 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 35:=======================>                               (87 + 2) / 200][Stage 35:==================================>                   (127 + 3) / 200][Stage 35:============================================>         (163 + 2) / 200]                                                                                [Stage 38:=========================>                             (91 + 2) / 200][Stage 38:===================================>                  (131 + 2) / 200][Stage 38:=============================================>        (170 + 2) / 200]                                                                                [Stage 41:=====================================>                (139 + 2) / 200][Stage 41:================================================>     (181 + 2) / 200]                                                                                [Stage 44:===================================>                  (130 + 2) / 200][Stage 44:================================================>     (180 + 3) / 200]                                                                                [I 2024-12-06 15:38:40.390 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
24/12/06 15:39:34 WARN CacheManager: Asked to cache already cached data.
24/12/06 15:39:55 WARN CacheManager: Asked to cache already cached data.
[I 2024-12-06 15:40:40.507 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:42:40.655 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:47:51.621 ServerApp] Saving file at /src/241206_DataFrameStart.ipynb
[I 2024-12-06 15:47:52.013 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:49bef3b0-fba4-46e4-9212-19136ae57d2b
[Stage 9:====================================>                  (134 + 2) / 200][Stage 9:=================================================>     (181 + 2) / 200]                                                                                [I 2024-12-06 15:48:42.642 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
24/12/06 15:50:04 WARN CacheManager: Asked to cache already cached data.
[Stage 3:============================>                          (103 + 2) / 200][Stage 3:=====================================>                 (137 + 2) / 200][Stage 3:=================================================>     (179 + 2) / 200]                                                                                [I 2024-12-06 15:50:42.774 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:52:42.896 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:54:43.652 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 15:58:44.670 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:03:25.209 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:13:02.307 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:14:58.067 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:16:58.207 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:18:58.654 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:20:58.796 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:21:33.800 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:22:01.489 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:23:33.931 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:24:15.497 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:26:15.646 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:28:15.804 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:30:15.949 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:30:41.788 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:31:46.488 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:31:50.383 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:32:51.136 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:34:51.293 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:35:10.806 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:36:39.159 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:36:40.427 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:38:39.320 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:40:39.463 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 118:============================================>        (168 + 2) / 200]                                                                                [Stage 120:==================================================>  (192 + 2) / 200]                                                                                [I 2024-12-06 16:42:39.613 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 122:================================================>    (184 + 3) / 200]                                                                                [Stage 124:=============================================>       (171 + 2) / 200]                                                                                [Stage 126:=================================================>   (186 + 2) / 200]                                                                                [I 2024-12-06 16:44:39.759 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 128:======================================>              (145 + 2) / 200]                                                                                [Stage 130:========================================>            (153 + 2) / 200]                                                                                [I 2024-12-06 16:45:02.191 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:46:04.275 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:48:04.421 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:50:04.577 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 154:============================================>        (167 + 2) / 200]                                                                                [I 2024-12-06 16:52:04.728 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:52:27.346 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 16:52:27.415 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 16:52:27.487 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 16:52:27.617 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:2d1e133f-9188-4454-92a1-b13a1c70b35c
[I 2024-12-06 16:52:27.617 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:55b4745f-c8e6-4c5e-b139-8eb9e497929a
[I 2024-12-06 16:53:40.806 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:54:57.862 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:56:58.027 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 186:====================================================>(199 + 1) / 200]                                                                                [I 2024-12-06 16:58:58.184 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 16:59:36.276 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 196:===================================================> (194 + 2) / 200]                                                                                [I 2024-12-06 17:01:30.254 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:03:18.400 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:03:19.098 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:15:38.577 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:15:56.263 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:16:13.620 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 234:=========================================>           (157 + 2) / 200]                                                                                [Stage 237:=========================================>           (157 + 2) / 200]                                                                                [I 2024-12-06 17:17:33.669 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:19:33.814 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:21:33.950 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[Stage 253:==================================================>  (192 + 3) / 200]                                                                                [Stage 260:=================================================>   (185 + 2) / 200]                                                                                [I 2024-12-06 17:23:34.080 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:24:27.871 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:24:40.706 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:26:06.603 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:26:27.491 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:26:34.365 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:26:52.531 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[IPKernelApp] ERROR | Failed to open SQLite history /home/lab17/.ipython/profile_default/history.sqlite (datatype mismatch).
[IPKernelApp] ERROR | History file was moved to /home/lab17/.ipython/profile_default/history-corrupt-2024-12-06T17.28.36.945050.sqlite and a new file created.
[I 2024-12-06 17:28:40.055 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:29:04.581 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 17:29:04.654 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 17:29:04.762 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 17:29:04.849 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:53fb995f-bd21-47d4-a143-b91859114609
[I 2024-12-06 17:29:04.858 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:4e77fd81-3da3-4f78-8a49-467f028be060
[I 2024-12-06 17:29:05.257 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 17:29:14.026 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:31:01.390 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:33:01.547 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:35:01.701 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:37:01.849 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:39:02.056 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
24/12/06 17:39:53 ERROR Executor: Exception in task 0.0 in stage 291.0 (TID 7979)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 594, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2916, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/opt/spark/python/pyspark/rdd.py", line 418, in func
    return f(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2144, in combineLocally
    merger.mergeValues(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py", line 240, in mergeValues
    for k, v in iterator:
ValueError: not enough values to unpack (expected 2, got 1)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/12/06 17:39:53 WARN TaskSetManager: Lost task 0.0 in stage 291.0 (TID 7979) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 594, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2916, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/opt/spark/python/pyspark/rdd.py", line 418, in func
    return f(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2144, in combineLocally
    merger.mergeValues(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py", line 240, in mergeValues
    for k, v in iterator:
ValueError: not enough values to unpack (expected 2, got 1)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

24/12/06 17:39:53 ERROR TaskSetManager: Task 0 in stage 291.0 failed 1 times; aborting job
24/12/06 17:39:56 ERROR Executor: Exception in task 0.0 in stage 294.0 (TID 7981)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 594, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2916, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/opt/spark/python/pyspark/rdd.py", line 418, in func
    return f(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2144, in combineLocally
    merger.mergeValues(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py", line 240, in mergeValues
    for k, v in iterator:
ValueError: not enough values to unpack (expected 2, got 1)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/12/06 17:39:56 WARN TaskSetManager: Lost task 0.0 in stage 294.0 (TID 7981) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 594, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2916, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/opt/spark/python/pyspark/rdd.py", line 418, in func
    return f(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2144, in combineLocally
    merger.mergeValues(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py", line 240, in mergeValues
    for k, v in iterator:
ValueError: not enough values to unpack (expected 2, got 1)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

24/12/06 17:39:56 ERROR TaskSetManager: Task 0 in stage 294.0 failed 1 times; aborting job
[I 2024-12-06 17:40:26.397 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 17:40:26.461 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 17:40:26.567 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 17:40:26.656 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:f43f78cc-983d-4499-8c5e-f9f1629d718e
[I 2024-12-06 17:40:26.815 ServerApp] Kernel started: 47fc89f0-9354-4b73-a8fc-f031296c42b4
[I 2024-12-06 17:40:27.333 ServerApp] Connecting to kernel 47fc89f0-9354-4b73-a8fc-f031296c42b4.
24/12/06 17:40:48 ERROR Executor: Exception in task 0.0 in stage 300.0 (TID 7986)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 594, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2916, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/opt/spark/python/pyspark/rdd.py", line 418, in func
    return f(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2144, in combineLocally
    merger.mergeValues(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py", line 240, in mergeValues
    for k, v in iterator:
ValueError: not enough values to unpack (expected 2, got 1)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
24/12/06 17:40:48 WARN TaskSetManager: Lost task 0.0 in stage 300.0 (TID 7986) (ip-172-31-3-9.ap-northeast-3.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 604, in main
    process()
  File "/opt/spark/python/lib/pyspark.zip/pyspark/worker.py", line 594, in process
    out_iter = func(split_index, iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2916, in pipeline_func
    return func(split, prev_func(split, iterator))
  File "/opt/spark/python/pyspark/rdd.py", line 418, in func
    return f(iterator)
  File "/opt/spark/python/pyspark/rdd.py", line 2144, in combineLocally
    merger.mergeValues(iterator)
  File "/opt/spark/python/lib/pyspark.zip/pyspark/shuffle.py", line 240, in mergeValues
    for k, v in iterator:
ValueError: not enough values to unpack (expected 2, got 1)

	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)
	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)
	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1209)
	at scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1215)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)
	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

24/12/06 17:40:48 ERROR TaskSetManager: Task 0 in stage 300.0 failed 1 times; aborting job
[I 2024-12-06 17:41:02.260 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:42:08.740 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:44:08.958 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:44:32.390 ServerApp] 302 GET / (@14.7.255.68) 0.40ms
[I 2024-12-06 17:44:32.429 JupyterNotebookApp] 302 GET /tree? (@14.7.255.68) 0.46ms
[I 2024-12-06 17:44:38.368 ServerApp] User ed20e5eada35421faa4871db02e9d79d logged in.
[I 2024-12-06 17:44:38.369 ServerApp] 302 POST /login?next=%2Ftree%3F (ed20e5eada35421faa4871db02e9d79d@14.7.255.68) 67.69ms
[I 2024-12-06 17:44:40.798 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 17:44:40.911 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 17:44:41.016 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 17:44:41.102 ServerApp] Connecting to kernel 47fc89f0-9354-4b73-a8fc-f031296c42b4.
[I 2024-12-06 17:44:41.155 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:7a2777df-3417-4828-8e82-59b8ef3f4120
[I 2024-12-06 17:44:53.266 ServerApp] Connecting to kernel 6062bbe8-0914-4ff4-b6c2-84e15218c129.
[I 2024-12-06 17:44:53.358 ServerApp] Connecting to kernel 40d08d52-c5da-4d7f-a7b5-45ef71c060ea.
[I 2024-12-06 17:44:53.497 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 17:44:53.603 ServerApp] Connecting to kernel 47fc89f0-9354-4b73-a8fc-f031296c42b4.
[I 2024-12-06 17:44:53.699 ServerApp] Starting buffering for 40d08d52-c5da-4d7f-a7b5-45ef71c060ea:43a3d15a-4b47-4763-9bb3-348c50b5654b
[I 2024-12-06 17:44:54.222 ServerApp] Connecting to kernel 69f7e184-143e-4325-a3b1-cc51e30ed732.
[I 2024-12-06 17:46:09.187 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:46:20.344 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:46:34.340 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:46:57.423 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:48:34.578 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:48:49.800 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:48:56.189 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:54:39.226 ServerApp] Saving file at /src/241206_DataFrameAPI.ipynb
[I 2024-12-06 17:54:40.377 ServerApp] Saving file at /src/241205_04_RDD_나이별친구수카운트.ipynb
[I 2024-12-06 17:54:41.118 ServerApp] Saving file at /src/241206_01_RDD_단어수세기.ipynb
[I 2024-12-06 17:54:41.499 ServerApp] Starting buffering for 6062bbe8-0914-4ff4-b6c2-84e15218c129:d40cc787-c033-496d-8236-81dad14d34ab
[I 2024-12-06 17:54:42.593 ServerApp] Starting buffering for 47fc89f0-9354-4b73-a8fc-f031296c42b4:85ea5a33-1c02-41a7-9440-007ba08276b8
[I 2024-12-06 17:54:43.021 ServerApp] Starting buffering for 69f7e184-143e-4325-a3b1-cc51e30ed732:d02b6027-8010-4bf6-9c94-19a601456db1
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[I 2024-12-13 17:07:02.473 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2024-12-13 17:07:02.478 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2024-12-13 17:07:02.483 ServerApp] jupyterlab | extension was successfully linked.
[W 2024-12-13 17:07:02.484 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2024-12-13 17:07:02.487 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2024-12-13 17:07:02.487 ServerApp] notebook | extension was successfully linked.
[I 2024-12-13 17:07:02.682 ServerApp] notebook_shim | extension was successfully linked.
[I 2024-12-13 17:07:02.717 ServerApp] notebook_shim | extension was successfully loaded.
[I 2024-12-13 17:07:02.719 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2024-12-13 17:07:02.720 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2024-12-13 17:07:02.722 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab
[I 2024-12-13 17:07:02.722 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/spark_start/share/jupyter/lab
[I 2024-12-13 17:07:02.722 LabApp] Extension Manager is 'pypi'.
[W 2024-12-13 17:07:02.722 LabApp] Failed to instantiate the extension manager pypi. Falling back to read-only manager.
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/labapp.py", line 837, in initialize_handlers
        ext_manager = manager_factory(app_options, listings_config, self)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/extensions/__init__.py", line 46, in get_pypi_manager
        return PyPIExtensionManager(app_options, ext_options, parent)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/extensions/pypi.py", line 134, in __init__
        self._httpx_client = httpx.AsyncClient(proxies=proxies)
    TypeError: __init__() got an unexpected keyword argument 'proxies'
[I 2024-12-13 17:07:02.726 ServerApp] jupyterlab | extension was successfully loaded.
[I 2024-12-13 17:07:02.730 ServerApp] notebook | extension was successfully loaded.
[I 2024-12-13 17:07:02.730 ServerApp] Serving notebooks from local directory: /home/lab17/git
[I 2024-12-13 17:07:02.730 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2024-12-13 17:07:02.730 ServerApp] http://ip-172-31-3-9:8917/tree
[I 2024-12-13 17:07:02.730 ServerApp]     http://127.0.0.1:8917/tree
[I 2024-12-13 17:07:02.730 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2024-12-13 17:07:02.746 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[W 2024-12-13 17:07:04.888 ServerApp] 404 GET /api/contents/git/src/241211_YellowTaxi2.ipynb?content=0&hash=1&1734077225966 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.42ms referer=http://15.168.86.105:8917/notebooks/git/src/241211_YellowTaxi2.ipynb
[W 2024-12-13 17:07:04.888 ServerApp] 404 GET /api/contents/git/src/241211_YellowTaxi2.ipynb?content=0&hash=1&1734077225966 (125.129.250.60): No such file or directory: git/src/241211_YellowTaxi2.ipynb
[I 2024-12-13 17:07:05.200 ServerApp] Uploading file to /git/src/241211_YellowTaxi2.ipynb
[W 2024-12-13 17:07:05.201 ServerApp] Notebook git/src/241211_YellowTaxi2.ipynb is not trusted
[W 2024-12-13 17:07:06.509 ServerApp] Notebook git/src/241211_YellowTaxi2.ipynb is not trusted
[I 2024-12-13 17:07:07.160 ServerApp] Kernel started: a904dd8d-227a-4189-83b2-537fafc413e0
[I 2024-12-13 17:07:07.604 ServerApp] Connecting to kernel a904dd8d-227a-4189-83b2-537fafc413e0.
[I 2024-12-13 17:07:07.685 ServerApp] Connecting to kernel a904dd8d-227a-4189-83b2-537fafc413e0.
[I 2024-12-13 17:07:07.754 ServerApp] Connecting to kernel a904dd8d-227a-4189-83b2-537fafc413e0.
[W 2024-12-13 17:07:14.195 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077235274 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:14.195 ServerApp] wrote error: 'Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8)
[W 2024-12-13 17:07:14.200 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077235274 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 5.04ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
24/12/13 17:07:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[W 2024-12-13 17:07:15.233 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:15.272 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 39.18ms referer=None
[W 2024-12-13 17:07:15.304 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077236385 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:15.305 ServerApp] wrote error: 'Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8)
[W 2024-12-13 17:07:15.306 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077236385 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.89ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[W 2024-12-13 17:07:16.193 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077237268 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:16.194 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:16.194 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077237268 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 2.26ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:17.220 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:17.222 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 2.64ms referer=None
[W 2024-12-13 17:07:17.250 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077238330 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:17.250 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:17.250 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077238330 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.06ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:18.224 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:18.225 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.98ms referer=None
[W 2024-12-13 17:07:18.259 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077239340 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:18.259 ServerApp] wrote error: 'Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8)
[W 2024-12-13 17:07:18.260 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077239340 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.77ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:20.227 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:20.229 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.47ms referer=None
[W 2024-12-13 17:07:20.268 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077241344 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:20.268 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:20.269 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077241344 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.68ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:22.223 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:22.224 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.89ms referer=None
[W 2024-12-13 17:07:22.276 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077243356 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:22.277 ServerApp] wrote error: 'Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8)
[W 2024-12-13 17:07:22.277 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077243356 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.75ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:23.197 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077244277 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:23.197 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:23.198 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077244277 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.79ms referer=http://15.168.86.105:8917/notebooks/git/src/241213_MLlib_ALS.ipynb
[W 2024-12-13 17:07:24.215 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:24.215 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.00ms referer=None
[W 2024-12-13 17:07:24.243 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077245324 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:24.243 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:24.244 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077245324 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.75ms referer=http://15.168.86.105:8917/notebooks/git/src/241213_MLlib_ALS.ipynb
[W 2024-12-13 17:07:25.227 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:25.228 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.83ms referer=None
[W 2024-12-13 17:07:25.262 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077246342 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:25.262 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:25.262 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077246342 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.72ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:26.216 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:26.217 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.69ms referer=None
[W 2024-12-13 17:07:26.244 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077247325 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:26.244 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:26.246 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077247325 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.97ms referer=http://15.168.86.105:8917/notebooks/git/src/241213_MLlib_ALS.ipynb
[W 2024-12-13 17:07:26.298 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:26.299 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.27ms referer=None
[W 2024-12-13 17:07:26.332 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077247412 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:26.332 ServerApp] wrote error: 'Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8)
[W 2024-12-13 17:07:26.332 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077247412 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.15ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:30.212 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:30.213 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.24ms referer=None
[W 2024-12-13 17:07:30.240 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077251321 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:30.240 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:30.242 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077251321 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.65ms referer=http://15.168.86.105:8917/notebooks/git/src/241213_MLlib_ALS.ipynb
[W 2024-12-13 17:07:32.216 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:32.217 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.04ms referer=None
[W 2024-12-13 17:07:32.250 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077253331 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:32.250 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:32.250 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077253331 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.09ms referer=http://15.168.86.105:8917/notebooks/git/src/241213_MLlib_ALS.ipynb
[W 2024-12-13 17:07:36.219 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:36.220 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.39ms referer=None
[W 2024-12-13 17:07:36.253 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077257334 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:36.253 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:36.254 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077257334 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.19ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:40.218 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:40.218 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=7d244fe8-72a4-4d1a-a4c3-55a79ce377d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.98ms referer=None
[W 2024-12-13 17:07:40.251 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077261332 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:40.251 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:40.252 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077261332 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.64ms referer=http://15.168.86.105:8917/notebooks/git/src/241213_MLlib_ALS.ipynb
[I 2024-12-13 17:07:41.799 ServerApp] Starting buffering for a904dd8d-227a-4189-83b2-537fafc413e0:e52c342f-3c95-4d7e-b9f8-013226ff55c9
[W 2024-12-13 17:07:42.215 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:42.216 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8/channels?session_id=18b8d86c-cc4e-4f62-80e6-6f2c755900d4 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.92ms referer=None
[W 2024-12-13 17:07:42.243 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077263324 (125.129.250.60): Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8
[W 2024-12-13 17:07:42.243 ServerApp] wrote error: 'Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 5c26d54c-8f48-48f4-91f7-c78092a3bdc8)
[W 2024-12-13 17:07:42.243 ServerApp] 404 GET /api/kernels/5c26d54c-8f48-48f4-91f7-c78092a3bdc8?1734077263324 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.79ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:44.224 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:44.226 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.58ms referer=None
[W 2024-12-13 17:07:44.253 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077265334 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:44.253 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:44.253 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077265334 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.25ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:48.213 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:48.213 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690/channels?session_id=566ac216-e82d-44c6-9f38-f393cc05b945 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.24ms referer=None
[W 2024-12-13 17:07:48.240 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077269322 (125.129.250.60): Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690
[W 2024-12-13 17:07:48.240 ServerApp] wrote error: 'Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/handlers.py", line 75, in get
        model = await ensure_async(km.kernel_model(kernel_id))
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 506, in kernel_model
        self._check_kernel_id(kernel_id)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/kernels/kernelmanager.py", line 537, in _check_kernel_id
        raise web.HTTPError(404, "Kernel does not exist: %s" % kernel_id)
    tornado.web.HTTPError: HTTP 404: Not Found (Kernel does not exist: 32c08da0-723b-40f6-a2f0-17f331fec690)
[W 2024-12-13 17:07:48.241 ServerApp] 404 GET /api/kernels/32c08da0-723b-40f6-a2f0-17f331fec690?1734077269322 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 0.82ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:57.701 ServerApp] 404 GET /api/contents/git/src/data/trips?content=1&hash=0&1734077278773 (bd481073dc2b4a5e922d23bf2ee30598@125.129.250.60) 1.25ms referer=http://15.168.86.105:8917/tree/git/src/data/trips
[W 2024-12-13 17:07:57.701 ServerApp] 404 GET /api/contents/git/src/data/trips?content=1&hash=0&1734077278773 (125.129.250.60): No such file or directory: git/src/data/trips
[I 2024-12-13 17:08:02.388 ServerApp] Connecting to kernel a904dd8d-227a-4189-83b2-537fafc413e0.
[I 2024-12-13 17:08:02.501 ServerApp] Starting buffering for a904dd8d-227a-4189-83b2-537fafc413e0:23559bca-3e5a-43be-94aa-278b957c8bc8
[W 2024-12-13 17:08:43.136 ServerApp] Notebook src/241211_YellowTaxi2.ipynb is not trusted
[I 2024-12-13 17:08:43.692 ServerApp] Connecting to kernel a904dd8d-227a-4189-83b2-537fafc413e0.
[W 2024-12-13 17:08:43.848 ServerApp] Notebook src/241211_YellowTaxi2.ipynb is not trusted
[I 2024-12-13 17:08:43.993 ServerApp] Starting buffering for a904dd8d-227a-4189-83b2-537fafc413e0:b7ed26a2-66e7-44c4-8985-511f99a06c24
[I 2024-12-13 17:08:44.497 ServerApp] Kernel started: f30c53c0-a19b-4bbd-bc9c-0416c7b58710
[I 2024-12-13 17:08:44.950 ServerApp] Connecting to kernel f30c53c0-a19b-4bbd-bc9c-0416c7b58710.
[I 2024-12-13 17:08:51.324 ServerApp] Starting buffering for f30c53c0-a19b-4bbd-bc9c-0416c7b58710:38534124-f313-41e2-bd39-9f46bc287fb4
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[IPKernelApp] WARNING | Parent appears to have exited, shutting down.
[I 2024-12-17 11:22:52.523 ServerApp] jupyter_lsp | extension was successfully linked.
[I 2024-12-17 11:22:52.528 ServerApp] jupyter_server_terminals | extension was successfully linked.
[I 2024-12-17 11:22:52.532 ServerApp] jupyterlab | extension was successfully linked.
[W 2024-12-17 11:22:52.535 JupyterNotebookApp] 'password' has moved from NotebookApp to ServerApp. This config will be passed to ServerApp. Be sure to update your config before our next release.
[W 2024-12-17 11:22:52.537 ServerApp] ServerApp.password config is deprecated in 2.0. Use PasswordIdentityProvider.hashed_password.
[I 2024-12-17 11:22:52.537 ServerApp] notebook | extension was successfully linked.
[I 2024-12-17 11:22:53.121 ServerApp] notebook_shim | extension was successfully linked.
[I 2024-12-17 11:22:53.228 ServerApp] notebook_shim | extension was successfully loaded.
[I 2024-12-17 11:22:53.229 ServerApp] jupyter_lsp | extension was successfully loaded.
[I 2024-12-17 11:22:53.230 ServerApp] jupyter_server_terminals | extension was successfully loaded.
[I 2024-12-17 11:22:53.248 LabApp] JupyterLab extension loaded from /home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab
[I 2024-12-17 11:22:53.248 LabApp] JupyterLab application directory is /home/ubuntu/anaconda3/envs/spark_start/share/jupyter/lab
[I 2024-12-17 11:22:53.249 LabApp] Extension Manager is 'pypi'.
[W 2024-12-17 11:22:53.249 LabApp] Failed to instantiate the extension manager pypi. Falling back to read-only manager.
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/labapp.py", line 837, in initialize_handlers
        ext_manager = manager_factory(app_options, listings_config, self)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/extensions/__init__.py", line 46, in get_pypi_manager
        return PyPIExtensionManager(app_options, ext_options, parent)
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyterlab/extensions/pypi.py", line 134, in __init__
        self._httpx_client = httpx.AsyncClient(proxies=proxies)
    TypeError: __init__() got an unexpected keyword argument 'proxies'
[I 2024-12-17 11:22:53.257 ServerApp] jupyterlab | extension was successfully loaded.
[I 2024-12-17 11:22:53.260 ServerApp] notebook | extension was successfully loaded.
[I 2024-12-17 11:22:53.261 ServerApp] Serving notebooks from local directory: /home/lab17/git
[I 2024-12-17 11:22:53.261 ServerApp] Jupyter Server 2.14.2 is running at:
[I 2024-12-17 11:22:53.261 ServerApp] http://ip-172-31-3-9:8917/tree
[I 2024-12-17 11:22:53.261 ServerApp]     http://127.0.0.1:8917/tree
[I 2024-12-17 11:22:53.261 ServerApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
[I 2024-12-17 11:22:53.277 ServerApp] Skipped non-installed server(s): bash-language-server, dockerfile-language-server-nodejs, javascript-typescript-langserver, jedi-language-server, julia-language-server, pyright, python-language-server, python-lsp-server, r-languageserver, sql-language-server, texlab, typescript-language-server, unified-language-server, vscode-css-languageserver-bin, vscode-html-languageserver-bin, vscode-json-languageserver-bin, yaml-language-server
[I 2024-12-17 11:27:30.512 ServerApp] 302 GET / (@125.129.250.60) 0.41ms
[I 2024-12-17 11:27:30.542 JupyterNotebookApp] 302 GET /tree? (@125.129.250.60) 0.49ms
[I 2024-12-17 11:27:34.215 ServerApp] User c3b6747a887346d69b5f478a5fd80001 logged in.
[I 2024-12-17 11:27:34.216 ServerApp] 302 POST /login?next=%2Ftree%3F (c3b6747a887346d69b5f478a5fd80001@125.129.250.60) 85.55ms
[I 2024-12-17 11:27:46.371 ServerApp] Creating new notebook in /src
[I 2024-12-17 11:27:46.852 ServerApp] Saving file at /src/Untitled.ipynb
[I 2024-12-17 11:27:46.982 ServerApp] Kernel started: e6d97556-d048-41a1-a464-866fb086cfbd
[I 2024-12-17 11:27:47.675 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 11:27:47.747 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 11:27:47.902 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 11:27:48.074 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 11:29:53.939 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:31:54.188 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:32:53.415 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:33:14.916 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:35:15.040 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:36:08.127 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:36:19.002 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:47:32.097 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:50:40.177 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 11:50:40.587 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 11:52:40.464 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:54:41.157 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 11:59:33.296 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 12:00:43.165 ServerApp] Saving file at /src/241217_mysql.ipynb
[W 2024-12-17 12:05:48.714 ServerApp] 400 GET /api/contents/src/mysql-connector-java-5.1.49.tar.gz?type=file&content=1&hash=1&format=text&1734404747892 (125.129.250.60): /home/lab17/git/src/mysql-connector-java-5.1.49.tar.gz is not UTF-8 encoded
[W 2024-12-17 12:05:48.714 ServerApp] wrote error: '/home/lab17/git/src/mysql-connector-java-5.1.49.tar.gz is not UTF-8 encoded'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/fileio.py", line 536, in _read_file
        (bcontent.decode("utf8"), "text", bcontent)
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/handlers.py", line 154, in get
        model = await ensure_async(
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_core/utils/__init__.py", line 198, in ensure_async
        result = await obj
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/filemanager.py", line 926, in get
        model = await self._file_model(
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/filemanager.py", line 835, in _file_model
        content, format, bytes_content = await self._read_file(os_path, format, raw=True)  # type: ignore[misc]
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/fileio.py", line 545, in _read_file
        raise HTTPError(
    tornado.web.HTTPError: HTTP 400: bad format (/home/lab17/git/src/mysql-connector-java-5.1.49.tar.gz is not UTF-8 encoded)
[W 2024-12-17 12:05:48.722 ServerApp] 400 GET /api/contents/src/mysql-connector-java-5.1.49.tar.gz?type=file&content=1&hash=1&format=text&1734404747892 (c3b6747a887346d69b5f478a5fd80001@125.129.250.60) 14.13ms referer=http://15.168.86.105:8917/tree/src
[I 2024-12-17 12:05:49.457 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[W 2024-12-17 12:05:49.512 ServerApp] 400 GET /api/contents/src/mysql-connector-java-5.1.49.tar.gz?type=file&content=1&hash=1&format=text&1734404748707 (125.129.250.60): /home/lab17/git/src/mysql-connector-java-5.1.49.tar.gz is not UTF-8 encoded
[W 2024-12-17 12:05:49.512 ServerApp] wrote error: '/home/lab17/git/src/mysql-connector-java-5.1.49.tar.gz is not UTF-8 encoded'
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/fileio.py", line 536, in _read_file
        (bcontent.decode("utf8"), "text", bcontent)
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte
    
    The above exception was the direct cause of the following exception:
    
    Traceback (most recent call last):
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/tornado/web.py", line 1790, in _execute
        result = await result
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/auth/decorator.py", line 73, in inner
        return await out
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/handlers.py", line 154, in get
        model = await ensure_async(
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_core/utils/__init__.py", line 198, in ensure_async
        result = await obj
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/filemanager.py", line 926, in get
        model = await self._file_model(
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/filemanager.py", line 835, in _file_model
        content, format, bytes_content = await self._read_file(os_path, format, raw=True)  # type: ignore[misc]
      File "/home/ubuntu/anaconda3/envs/spark_start/lib/python3.8/site-packages/jupyter_server/services/contents/fileio.py", line 545, in _read_file
        raise HTTPError(
    tornado.web.HTTPError: HTTP 400: bad format (/home/lab17/git/src/mysql-connector-java-5.1.49.tar.gz is not UTF-8 encoded)
[W 2024-12-17 12:05:49.512 ServerApp] 400 GET /api/contents/src/mysql-connector-java-5.1.49.tar.gz?type=file&content=1&hash=1&format=text&1734404748707 (c3b6747a887346d69b5f478a5fd80001@125.129.250.60) 2.51ms referer=http://15.168.86.105:8917/edit/src/mysql-connector-java-5.1.49.tar.gz
[I 2024-12-17 12:06:46.142 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 12:08:47.138 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 13:21:24.749 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 13:21:29.214 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 13:26:42.697 ServerApp] Copying src/241217_mysql.ipynb to /src
[I 2024-12-17 13:26:51.755 ServerApp] Connecting to kernel e6d97556-d048-41a1-a464-866fb086cfbd.
[I 2024-12-17 13:26:52.134 ServerApp] Kernel started: dac045f9-793d-4b83-b175-84ef33d9ec27
[I 2024-12-17 13:26:52.581 ServerApp] Connecting to kernel dac045f9-793d-4b83-b175-84ef33d9ec27.
[I 2024-12-17 13:28:14.122 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:30:14.658 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:32:14.800 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
24/12/17 13:32:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Tue Dec 17 13:32:52 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:33:06 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 13:34:14.920 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:34:33.901 ServerApp] Saving file at /src/241217_mysql.ipynb
[I 2024-12-17 13:34:34.092 ServerApp] Starting buffering for e6d97556-d048-41a1-a464-866fb086cfbd:ced93490-53ea-43ed-991f-e1b6fb981469
[Stage 0:>                                                          (0 + 1) / 1]Tue Dec 17 13:34:52 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
                                                                                [I 2024-12-17 13:34:53.780 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 13:35:22 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:35:23 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:35:39 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:35:40 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:35:56 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:36:24 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:36:24 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 13:36:53.901 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:38:54.035 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:40:54.162 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 13:41:12 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:41:12 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:41:29 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:41:29 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 13:42:55.031 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:50:58.033 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 13:52:34 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 13:52:58.166 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 13:53:05 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:53:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:53:11 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:54:08 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:54:08 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 13:54:58.309 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 13:55:02 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:02 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:34 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:34 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:50 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:50 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:53 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:55:53 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:56:05 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:56:05 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:56:16 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:56:16 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 13:56:59.022 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 13:58:59.146 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 13:59:13 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 13:59:13 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 14:01:00.039 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:03:00.166 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 14:03:04 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:10 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:11 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:11 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:03:11 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:04:34 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 14:05:00.294 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:07:00.434 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:30 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
Tue Dec 17 14:07:42 KST 2024 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
[I 2024-12-17 14:07:47.067 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:07:54.995 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:08:06.122 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:08:47.746 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:28:41.790 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 14:28:55.837 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 15:53:50.883 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:13:18.923 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:15:16.713 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:15:51.711 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:17:17.129 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:19:17.570 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:19:21.121 ServerApp] Saving file at /src/241217_mysql_pyspark.ipynb
[I 2024-12-17 18:19:23.103 ServerApp] Starting buffering for dac045f9-793d-4b83-b175-84ef33d9ec27:4a55ef17-bf86-489b-ac39-a72474aec300
[C 2024-12-17 18:21:32.163 ServerApp] received signal 15, stopping
[I 2024-12-17 18:21:32.163 ServerApp] Shutting down 5 extensions
[I 2024-12-17 18:21:32.165 ServerApp] Shutting down 2 kernels
[I 2024-12-17 18:21:32.165 ServerApp] Kernel shutdown: dac045f9-793d-4b83-b175-84ef33d9ec27
[I 2024-12-17 18:21:32.166 ServerApp] Kernel shutdown: e6d97556-d048-41a1-a464-866fb086cfbd
